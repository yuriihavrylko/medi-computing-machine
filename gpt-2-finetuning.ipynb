{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.replace(\"XX.XX.XXXX\", \" \")\n",
    "    s = s.replace(\"XX letá\", \" \")\n",
    "    s = s.replace(\"XX-letá\", \" \")\n",
    "    s = s.replace(\"XX-letý\", \" \")\n",
    "    s = s.replace(\"XX letý\", \" \")\n",
    "    s = s.replace(\"XX8letá\", \" \")\n",
    "    s = s.replace(\"XX9letá\", \" \")\n",
    "    s = s.replace(\"XX7letá\", \" \")\n",
    "    s = s.replace(\"XX6letá\", \" \")\n",
    "    s = s.replace(\"XX5letá\", \" \")\n",
    "    s = s.replace(\"XX4letá\", \" \")\n",
    "    s = s.replace(\"XX3letá\", \" \")\n",
    "    s = s.replace(\"XX2letá\", \" \")\n",
    "    s = s.replace(\"XX\", \" \")\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_form_full_word = {\n",
    "    \"tj.\": \"to jest\",\n",
    "    \"srd.\": \"srdce\",\n",
    "    \"sed.\": \"sedačka\",\n",
    "    \"kad.\": \"kadeřník\",\n",
    "    \"rec.\": \"recepce\",\n",
    "    \"lab.\": \"laboratoř\",\n",
    "    \"hod.\": \"hodina\",\n",
    "    \"tik.\": \"tikání\",\n",
    "    \"hd.\": \"hodina\",\n",
    "    \"lev.\": \"levý\",\n",
    "    \"via.\": \"viadukt\",\n",
    "    \"výn.\": \"výnos\",\n",
    "    \"abd.\": \"abdomen\",\n",
    "    \"red.\": \"redakce\",\n",
    "    \"u.\": \"ulice\",\n",
    "    \"zn.\": \"znak\",\n",
    "    \"anm.\": \"anotace\",\n",
    "    \"kys.\": \"kyselina\",\n",
    "    \"inz.\": \"inženýr\",\n",
    "    \"ery.\": \"erytrocyt\",\n",
    "    \"gr.\": \"gram\",\n",
    "    \"abn.\": \"abnormalita\",\n",
    "    \"fyz.\": \"fyzika\",\n",
    "    \"alb.\": \"album\",\n",
    "    \"asc.\": \"asceze\",\n",
    "    \"tam.\": \"tampon\",\n",
    "    \"cíl.\": \"cíl\",\n",
    "    \"res.\": \"rezistence\",\n",
    "    \"aa.\": \"anonymní alkoholici\",\n",
    "    \"gyn.\": \"gynokologie\",\n",
    "    \"tjn.\": \"to je\",\n",
    "    \"chl.\": \"chlazení\",\n",
    "    \"soc.\": \"sociologie\",\n",
    "    \"obd.\": \"oběd\",\n",
    "    \"peč.\": \"pečení\",\n",
    "    \"inc.\": \"inkaso\",\n",
    "    \"sp.\": \"spolek\",\n",
    "    \"den.\": \"denní\",\n",
    "    \"rg.\": \"regulátor\",\n",
    "    \"č.\": \"číslo\",\n",
    "    \"bol.\": \"bolest\",\n",
    "    \"i.\": \"iota\",\n",
    "    \"sál.\": \"sál\",\n",
    "    \"k.\": \"klient\",\n",
    "    \"dx.\": \"diagnóza\",\n",
    "    \"pat.\": \"patologie\",\n",
    "    \"exp.\": \"experiment\",\n",
    "    \"imp.\": \"import\",\n",
    "    \"inj.\": \"injekce\",\n",
    "    \"biv.\": \"bivak\",\n",
    "    \"tzn.\": \"to znamená\",\n",
    "    \"bnp.\": \"brain natriuretic peptide\",\n",
    "    \"z.\": \"zákaz\",\n",
    "    \"str.\": \"strana\",\n",
    "    \"jod.\": \"jod\",\n",
    "    \"bb.\": \"base ball\",\n",
    "    \"no.\": \"norma\",\n",
    "    \"sn.\": \"sníh\",\n",
    "    \"bio.\": \"biologie\",\n",
    "    \"tis.\": \"tisíc\",\n",
    "    \"dyn.\": \"dynamika\",\n",
    "    \"gen.\": \"generál\",\n",
    "    \"bpm.\": \"beats per minute\",\n",
    "    \"def.\": \"definice\",\n",
    "    \"sys.\": \"systém\",\n",
    "    \"dig.\": \"digitální\",\n",
    "    \"end.\": \"endokrinologie\",\n",
    "    \"moč.\": \"moč\",\n",
    "    \"akc.\": \"akcie\",\n",
    "    \"am.\": \"ampér\",\n",
    "    \"záň.\": \"zápal\",\n",
    "    \"řeš.\": \"řešení\",\n",
    "    \"spt.\": \"sport\",\n",
    "    \"obj.\": \"objekt\",\n",
    "    \"doc.\": \"doktor\",\n",
    "    \"exc.\": \"excitace\",\n",
    "    \"ev.\": \"evangelium\",\n",
    "    \"měs.\": \"měsíc\",\n",
    "    \"nej.\": \"nejvyšší\",\n",
    "    \"pp.\": \"popis\",\n",
    "    \"ak.\": \"akademik\",\n",
    "    \"pr.\": \"práce\",\n",
    "    \"ant.\": \"anténa\",\n",
    "    \"let.\": \"letiště\",\n",
    "    \"adj.\": \"adjektivum\",\n",
    "    \"rod.\": \"rodina\",\n",
    "    \"os.\": \"osoba\",\n",
    "    \"map.\": \"mapa\",\n",
    "    \"pln.\": \"plný\",\n",
    "    \"tek.\": \"tekutina\",\n",
    "    \"il.\": \"ilustrace\",\n",
    "    \"acs.\": \"akutní koronární syndrom\",\n",
    "    \"bok.\": \"bok\",\n",
    "    \"com.\": \"komunikace\",\n",
    "    \"amb.\": \"ambulance\",\n",
    "    \"již.\": \"již\",\n",
    "    \"lok.\": \"lokace\",\n",
    "    \"tbl.\": \"tabletka\",\n",
    "    \"kor.\": \"koruna\",\n",
    "    \"seq.\": \"sekvenční\",\n",
    "    \"dis.\": \"distribuce\",\n",
    "    \"mab.\": \"monoklonální protilátky\",\n",
    "    \"kol.\": \"koleno\",\n",
    "    \"sv.\": \"svatý\",\n",
    "    \"neg.\": \"negativní\",\n",
    "    \"dop.\": \"dopis\",\n",
    "    \"zem.\": \"země\",\n",
    "    \"gap.\": \"gap\",\n",
    "    \"dfi.\": \"deficit\",\n",
    "    \"bak.\": \"bakterie\",\n",
    "    \"kt.\": \"kategorický\",\n",
    "    \"vč.\": \"včetně\",\n",
    "    \"vys.\": \"vysoký\",\n",
    "    \"dif.\": \"diferenciál\",\n",
    "    \"reg.\": \"regulace\",\n",
    "    \"o.\": \"okruh\",\n",
    "    \"ung.\": \"unguentum\",\n",
    "    \"trž.\": \"tržiště\",\n",
    "    \"gmg.\": \"grammage\",\n",
    "    \"omp.\": \"omeprazol\",\n",
    "    \"dní.\": \"denně\",\n",
    "    \"hor.\": \"hory\",\n",
    "    \"rpt.\": \"report\",\n",
    "    \"bas.\": \"basa\",\n",
    "    \"gel.\": \"gel\",\n",
    "    \"pri.\": \"primární\",\n",
    "    \"aot.\": \"aorta\",\n",
    "    \"amp.\": \"ampér\",\n",
    "    \"opk.\": \"opravná\",\n",
    "    \"ca.\": \"cirka\",\n",
    "    \"n.\": \"nomenklatura\",\n",
    "    \"mut.\": \"mutace\",\n",
    "    \"by.\": \"bývalý\",\n",
    "    \"tib.\": \"tibia\",\n",
    "    \"hr.\": \"hranice\",\n",
    "    \"gg.\": \"geologie\",\n",
    "    \"mol.\": \"molekula\",\n",
    "    \"vit.\": \"vitamín\",\n",
    "    \"dny.\": \"dna\",\n",
    "    \"sup.\": \"suplement\",\n",
    "    \"org.\": \"organizace\",\n",
    "    \"pul.\": \"pulz\",\n",
    "    \"tac.\": \"taktický\",\n",
    "    \"jíč.\": \"jícnový\",\n",
    "    \"oka.\": \"oko\",\n",
    "    \"tx.\": \"transplantace\",\n",
    "    \"j.\": \"jih\",\n",
    "    \"ch.\": \"chirurgie\",\n",
    "    \"tot.\": \"totální\",\n",
    "    \"mm.\": \"milimetr\",\n",
    "    \"atb.\": \"antibiotika\",\n",
    "    \"p.\": \"para\",\n",
    "    \"min.\": \"minimální\",\n",
    "    \"tř.\": \"třída\",\n",
    "    \"trk.\": \"trafika\",\n",
    "    \"fc.\": \"football club\",\n",
    "    \"car.\": \"karcinom\",\n",
    "    \"baz.\": \"báze\",\n",
    "    \"vln.\": \"vlnění\",\n",
    "    \"dil.\": \"dilatace\",\n",
    "    \"vr.\": \"vrcholek\",\n",
    "    \"sín.\": \"síně\",\n",
    "    \"ext.\": \"extremum\",\n",
    "    \"kp.\": \"kapesník\",\n",
    "    \"pův.\": \"původ\",\n",
    "    \"t.\": \"tunel\",\n",
    "    \"ost.\": \"ostrov\",\n",
    "    \"sk.\": \"sklizeň\",\n",
    "    \"šok.\": \"šok\",\n",
    "    \"stř.\": \"středa\",\n",
    "    \"sek.\": \"sekce\",\n",
    "    \"th.\": \"těhotná\",\n",
    "    \"pl.\": \"plast\",\n",
    "    \"fib.\": \"fibula\",\n",
    "    \"bl.\": \"bílý\",\n",
    "    \"těž.\": \"těžký\",\n",
    "    \"zde.\": \"zde\",\n",
    "    \"ren.\": \"renovace\",\n",
    "    \"mir.\": \"miráž\",\n",
    "    \"on.\": \"onkologie\",\n",
    "    \"ep.\": \"epizoda\",\n",
    "    \"re.\": \"rehabilitace\",\n",
    "    \"fr.\": \"frame\",\n",
    "    \"tl.\": \"tělo\",\n",
    "    \"srd.\": \"srdce\",\n",
    "    \"sed.\": \"sedadlo\",\n",
    "    \"kad.\": \"kandidát\",\n",
    "    \"rec.\": \"recepce\",\n",
    "    \"lab.\": \"laboratoř\",\n",
    "    \"hod.\": \"hodina\",\n",
    "    \"tik.\": \"tiket\",\n",
    "    \"hd.\": \"hudba\",\n",
    "    \"lev.\": \"levný\",\n",
    "    \"via.\": \"víceúčelový\",\n",
    "    \"výn.\": \"výnosek\",\n",
    "    \"abd.\": \"abdomen\",\n",
    "    \"red.\": \"redakce\",\n",
    "    \"u.\": \"ulice\",\n",
    "    \"zn.\": \"znak\",\n",
    "    \"anm.\": \"anamnéza\",\n",
    "    \"kys.\": \"kyselina\",\n",
    "    \"inz.\": \"inženýr\",\n",
    "    \"ery.\": \"erytrocyt\",\n",
    "    \"gr.\": \"gram\",\n",
    "    \"abn.\": \"abnormální\",\n",
    "    \"fyz.\": \"fyzika\",\n",
    "    \"alb.\": \"albumin\",\n",
    "    \"asc.\": \"ascendentní\",\n",
    "    \"tam.\": \"tampon\",\n",
    "    \"cíl.\": \"cílený\",\n",
    "    \"res.\": \"respirace\",\n",
    "    \"aa.\": \"aminokyselina\",\n",
    "    \"gyn.\": \"gynekologie\",\n",
    "    \"tjn.\": \"tajný\",\n",
    "    \"chl.\": \"chlor\",\n",
    "    \"soc.\": \"sociální\",\n",
    "    \"obd.\": \"obvod\",\n",
    "    \"peč.\": \"pečeť\",\n",
    "    \"inc.\": \"incidence\",\n",
    "    \"sp.\": \"speciální\",\n",
    "    \"den.\": \"deník\",\n",
    "    \"rg.\": \"rentgen\",\n",
    "    \"č.\": \"číslo\",\n",
    "    \"bol.\": \"bole\",\n",
    "    \"i.\": \"infekce\",\n",
    "    \"sál.\": \"sál\",\n",
    "    \"k.\": \"kvalita\",\n",
    "    \"dx.\": \"diagnóza\",\n",
    "    \"pat.\": \"patologie\",\n",
    "    \"exp.\": \"experiment\",\n",
    "    \"imp.\": \"import\",\n",
    "    \"inj.\": \"injekce\",\n",
    "    \"biv.\": \"bivalentní\",\n",
    "    \"tzn.\": \"to znamená\",\n",
    "    \"bnp.\": \"brain natriuretic peptide\",\n",
    "    \"z.\": \"země\",\n",
    "    \"str.\": \"stránka\",\n",
    "    \"jod.\": \"jód\",\n",
    "    \"bb.\": \"bílá krvinka\",\n",
    "    \"no.\": \"číslo\",\n",
    "    \"sn.\": \"snížení\",\n",
    "    \"bio.\": \"biologie\",\n",
    "    \"tis.\": \"tkáň\",\n",
    "    \"dyn.\": \"dynamický\",\n",
    "    \"gen.\": \"generace\",\n",
    "    \"bpm.\": \"bicykly za minutu\",\n",
    "    \"def.\": \"definice\",\n",
    "    \"sys.\": \"systém\",\n",
    "    \"dig.\": \"digitální\",\n",
    "    \"end.\": \"endokrinologie\",\n",
    "    \"moč.\": \"moč\",\n",
    "    \"akc.\": \"akce\",\n",
    "    \"am.\": \"americký\",\n",
    "    \"záň.\": \"zákon\",\n",
    "    \"řeš.\": \"řešení\",\n",
    "    \"spt.\": \"sport\",\n",
    "    \"obj.\": \"objekt\",\n",
    "    \"doc.\": \"dokument\",\n",
    "    \"exc.\": \"excelentní\",\n",
    "    \"ev.\": \"event\",\n",
    "    \"měs.\": \"měsíc\",\n",
    "    \"nej.\": \"nejlepší\",\n",
    "    \"pp.\": \"poštovní přihrádka\",\n",
    "    \"ak.\": \"akademie\",\n",
    "    \"pr.\": \"praxe\",\n",
    "    \"ant.\": \"antibiotikum\",\n",
    "    \"let.\": \"letadlo\",\n",
    "    \"adj.\": \"adjective\",\n",
    "    \"rod.\": \"rodina\",\n",
    "    \"os.\": \"osoba\",\n",
    "    \"map.\": \"mapa\",\n",
    "    \"pln.\": \"plný\",\n",
    "    \"tek.\": \"tekutina\",\n",
    "    \"il.\": \"ilustrace\",\n",
    "    \"acs.\": \"akutní koronární syndrom\",\n",
    "    \"bok.\": \"bok\",\n",
    "    \"com.\": \"komunikace\",\n",
    "    \"amb.\": \"ambulance\",\n",
    "    \"již.\": \"jižní\",\n",
    "    \"lok.\": \"lokalizace\",\n",
    "    \"tbl.\": \"tabulka\",\n",
    "    \"kor.\": \"korelace\",\n",
    "    \"seq.\": \"sekvence\",\n",
    "    \"dis.\": \"dislokace\",\n",
    "    \"mab.\": \"monoklonální protilátky\",\n",
    "    \"kol.\": \"kolegium\",\n",
    "    \"sv.\": \"světlo\",\n",
    "    \"neg.\": \"negativní\",\n",
    "    \"dop.\": \"doporučený\",\n",
    "    \"zem.\": \"země\",\n",
    "    \"gap.\": \"gap\",\n",
    "    \"dfi.\": \"dětské fixy\",\n",
    "    \"bak.\": \"bakterie\",\n",
    "    \"kt.\": \"kultura\",\n",
    "    \"vč.\": \"včera\",\n",
    "    \"vys.\": \"vysoký\",\n",
    "    \"dif.\": \"diferenciace\",\n",
    "    \"reg.\": \"registrace\",\n",
    "    \"o.\": \"okres\",\n",
    "    \"ung.\": \"unguentum\",\n",
    "    \"trž.\": \"tržba\",\n",
    "    \"gmg.\": \"gaming\",\n",
    "    \"omp.\": \"omphalos\",\n",
    "    \"dní.\": \"dní\",\n",
    "    \"hor.\": \"horizont\",\n",
    "    \"rpt.\": \"repeat\",\n",
    "    \"bas.\": \"basofil\",\n",
    "    \"gel.\": \"gelový\",\n",
    "    \"pri.\": \"priorita\",\n",
    "    \"aot.\": \"aorta\",\n",
    "    \"amp.\": \"amplifikace\",\n",
    "    \"opk.\": \"optický klam\",\n",
    "    \"ca.\": \"cancer\",\n",
    "    \"n.\": \"národní\",\n",
    "    \"mut.\": \"mutace\",\n",
    "    \"by.\": \"bylina\",\n",
    "    \"tib.\": \"tibetský\",\n",
    "    \"hr.\": \"horečka\",\n",
    "    \"gg.\": \"good game\",\n",
    "    \"mol.\": \"molekula\",\n",
    "    \"vit.\": \"vitamín\",\n",
    "    \"dny.\": \"dny\",\n",
    "    \"sup.\": \"suplement\",\n",
    "    \"org.\": \"organizace\",\n",
    "    \"pul.\": \"pulz\",\n",
    "    \"tac.\": \"tachykardie\",\n",
    "    \"jíc.\": \"jícnový\",\n",
    "    \"oka.\": \"okamžitě\",\n",
    "    \"tx.\": \"terapie\",\n",
    "    \"j.\": \"jednotka\",\n",
    "    \"ch.\": \"chirurgie\",\n",
    "    \"tot.\": \"totalita\",\n",
    "    \"mm.\": \"milimetr\",\n",
    "    \"atb.\": \"antibiotikum\",\n",
    "    \"p.\": \"poloha\",\n",
    "    \"min.\": \"minuta\",\n",
    "    \"tř.\": \"třída\",\n",
    "    \"trk.\": \"track\",\n",
    "    \"fc.\": \"fotbalový klub\",\n",
    "    \"car.\": \"carnivora\",\n",
    "    \"baz.\": \"bazilika\",\n",
    "    \"vln.\": \"vlna\",\n",
    "    \"dil.\": \"diluce\",\n",
    "    \"vr.\": \"vrstva\",\n",
    "    \"sín.\": \"síň\",\n",
    "    \"ext.\": \"extrémní\",\n",
    "    \"kp.\": \"krevní tlak\",\n",
    "    \"pův.\": \"původ\",\n",
    "    \"t.\": \"tento\",\n",
    "    \"ost.\": \"ostatní\",\n",
    "    \"sk.\": \"sklep\",\n",
    "    \"šok.\": \"šok\",\n",
    "    \"stř.\": \"střední\",\n",
    "    \"sek.\": \"sekunda\",\n",
    "    \"th.\": \"thorax\",\n",
    "    \"pl.\": \"pláž\",\n",
    "    \"fib.\": \"fibrilace\",\n",
    "    \"bl.\": \"blecha\",\n",
    "    \"těž.\": \"těžký\",\n",
    "    \"zde.\": \"zde\",\n",
    "    \"ren.\": \"renální\",\n",
    "    \"mir.\": \"míra\",\n",
    "    \"on.\": \"onkologie\",\n",
    "    \"ep.\": \"epizoda\",\n",
    "    \"re.\": \"reálný\",\n",
    "    \"fr.\": \"frakce\",\n",
    "    \"tl.\": \"tlak\",\n",
    "    \"cm.\": \"centimetr\",\n",
    "    \"for.\": \"forenzní\",\n",
    "    \"lož.\": \"ložnice\",\n",
    "    \"noč.\": \"noční\",\n",
    "    \"ill.\": \"ilustrace\",\n",
    "    \"syn.\": \"synonymum\",\n",
    "    \"bic.\": \"biceps\",\n",
    "    \"krk.\": \"krk\",\n",
    "    \"fam.\": \"familie\",\n",
    "    \"dev.\": \"development\",\n",
    "    \"kr.\": \"krev\",\n",
    "    \"kom.\": \"komise\",\n",
    "    \"př.\": \"příklad\",\n",
    "    \"vyš.\": \"vyšetření\",\n",
    "    \"vad.\": \"vada\",\n",
    "    \"hep.\": \"hepatitida\",\n",
    "    \"spf.\": \"střední proudový filtr\",\n",
    "    \"eti.\": \"etický\",\n",
    "    \"tok.\": \"tok\",\n",
    "    \"sat.\": \"satelit\",\n",
    "    \"gl.\": \"glándula\",\n",
    "    \"vág.\": \"vágus\",\n",
    "    \"nás.\": \"následuje\",\n",
    "    \"dol.\": \"dolní\",\n",
    "    \"inv.\": \"invaze\",\n",
    "    \"jat.\": \"jaterní\",\n",
    "    \"fat.\": \"fáze\",\n",
    "    \"ekv.\": \"ekvivalent\",\n",
    "    \"tp.\": \"typ\",\n",
    "    \"sín.\": \"síň\",\n",
    "    \"om.\": \"omáčka\",\n",
    "    \"chr.\": \"chránič\",\n",
    "    \"do.\": \"dokončit\",\n",
    "    \"lat.\": \"latinský\",\n",
    "    \"ad.\": \"adresát\",\n",
    "    \"odd.\": \"oddíl\",\n",
    "    \"nám.\": \"náměstí\",\n",
    "    \"kat.\": \"kategorie\",\n",
    "    \"fce.\": \"funkce\",\n",
    "    \"odp.\": \"odpověď\",\n",
    "    \"bíl.\": \"bílý\",\n",
    "    \"por.\": \"porucha\",\n",
    "    \"noc.\": \"noc\",\n",
    "    \"bif.\": \"bifurkace\",\n",
    "    \"byl.\": \"bylinný\",\n",
    "    \"koz.\": \"koza\",\n",
    "    \"ko.\": \"kouření\",\n",
    "    \"sc.\": \"scéna\",\n",
    "    \"žl.\": \"žlutý\",\n",
    "    \"ex.\": \"exemplář\",\n",
    "    \"těž.\": \"těžký\",\n",
    "    \"tr.\": \"trénink\",\n",
    "    \"dne.\": \"dnešní\",\n",
    "    \"si.\": \"sice\",\n",
    "    \"sel.\": \"selektivní\",\n",
    "    \"dok.\": \"dokumentace\",\n",
    "    \"osy.\": \"osy\",\n",
    "    \"amn.\": \"amen\",\n",
    "    \"kl.\": \"klid\",\n",
    "    \"pot.\": \"potenciál\",\n",
    "    \"int.\": \"inteligence\",\n",
    "    \"zh.\": \"zhruba\",\n",
    "    \"alg.\": \"algebra\",\n",
    "    \"r.\": \"rok\",\n",
    "    \"epi.\": \"epidemiologie\",\n",
    "    \"met.\": \"metabolismus\",\n",
    "    \"vs.\": \"versus\",\n",
    "    \"od.\": \"odezva\",\n",
    "    \"glu.\": \"glukóza\",\n",
    "    \"mal.\": \"malý\",\n",
    "    \"úzk.\": \"úzkost\",\n",
    "    \"dr.\": \"doktor\",\n",
    "    \"f.\": \"funkce\",\n",
    "    \"bp.\": \"bod\",\n",
    "    \"sin.\": \"sinus\",\n",
    "    \"bat.\": \"baterie\",\n",
    "    \"veň.\": \"večeře\",\n",
    "    \"sep.\": \"separace\",\n",
    "    \"nos.\": \"nosní\",\n",
    "    \"sec.\": \"sekunda\",\n",
    "    \"op.\": \"opakovaný\",\n",
    "    \"s.\": \"současný\",\n",
    "    \"cps.\": \"cyklopentasiloxan\",\n",
    "    \"art.\": \"artikulace\",\n",
    "    \"nem.\": \"nemoc\",\n",
    "    \"vid.\": \"video\",\n",
    "    \"žíl.\": \"žíla\",\n",
    "    \"poč.\": \"počet\",\n",
    "    \"ao.\": \"auto\",\n",
    "    \"jug.\": \"jugo\",\n",
    "    \"fem.\": \"femininum\",\n",
    "    \"ac.\": \"acidum\",\n",
    "    \"a.\": \"aorta\",\n",
    "    \"obl.\": \"oblast\",\n",
    "    \"c.\": \"centrum\",\n",
    "    \"rok.\": \"rokina\",\n",
    "    \"vel.\": \"velký\",\n",
    "    \"vag.\": \"vaginální\",\n",
    "    \"med.\": \"medicína\",\n",
    "    \"kmp.\": \"kompas\",\n",
    "    \"kon.\": \"končetina\",\n",
    "    \"moz.\": \"mozeček\",\n",
    "    \"cm.\": \"centimetr\",\n",
    "    \"for.\": \"formace\",\n",
    "    \"lož.\": \"ložisko\",\n",
    "    \"noč.\": \"noční\",\n",
    "    \"ill.\": \"ilustrace\",\n",
    "    \"syn.\": \"syn\",\n",
    "    \"bic.\": \"bicí nástroj\",\n",
    "    \"krk.\": \"krk\",\n",
    "    \"fam.\": \"familie\",\n",
    "    \"dev.\": \"deviace\",\n",
    "    \"kr.\": \"král\",\n",
    "    \"kom.\": \"komentář\",\n",
    "    \"př.\": \"příklad\",\n",
    "    \"vyš.\": \"vyšetření\",\n",
    "    \"vad.\": \"vada\",\n",
    "    \"hep.\": \"hepatitida\",\n",
    "    \"spf.\": \"sluneční ochranný faktor\",\n",
    "    \"eti.\": \"etiologie\",\n",
    "    \"tok.\": \"tok\",\n",
    "    \"sat.\": \"saturace\",\n",
    "    \"gl.\": \"glukóza\",\n",
    "    \"vág.\": \"vágní\",\n",
    "    \"nás.\": \"následující\",\n",
    "    \"dol.\": \"dolar\",\n",
    "    \"inv.\": \"investice\",\n",
    "    \"jat.\": \"jaterní\",\n",
    "    \"fat.\": \"fatální\",\n",
    "    \"ekv.\": \"ekvivalent\",\n",
    "    \"tp.\": \"teplota\",\n",
    "    \"síň.\": \"síně\",\n",
    "    \"om.\": \"ohm\",\n",
    "    \"chr.\": \"chrám\",\n",
    "    \"do.\": \"domácí\",\n",
    "    \"lat.\": \"latina\",\n",
    "    \"ad.\": \"advertise\",\n",
    "    \"odd.\": \"oddělení\",\n",
    "    \"nám.\": \"náměstí\",\n",
    "    \"kat.\": \"kategorie\",\n",
    "    \"fce.\": \"funkce\",\n",
    "    \"odp.\": \"odpověď\",\n",
    "    \"bíl.\": \"bílý\",\n",
    "    \"por.\": \"porada\",\n",
    "    \"noc.\": \"noc\",\n",
    "    \"bif.\": \"bifteck\",\n",
    "    \"byl.\": \"bylina\",\n",
    "    \"koz.\": \"kozel\",\n",
    "    \"ko.\": \"kobalt\",\n",
    "    \"sc.\": \"scéna\",\n",
    "    \"žl.\": \"žluč\",\n",
    "    \"ex.\": \"example\",\n",
    "    \"těž.\": \"těžký\",\n",
    "    \"tr.\": \"trenér\",\n",
    "    \"dne.\": \"dnes\",\n",
    "    \"si.\": \"síran\",\n",
    "    \"sel.\": \"selen\",\n",
    "    \"dok.\": \"dokument\",\n",
    "    \"osy.\": \"osy\",\n",
    "    \"amn.\": \"amoniak\",\n",
    "    \"kl.\": \"klíč\",\n",
    "    \"pot.\": \"potenciál\",\n",
    "    \"int.\": \"intenzita\",\n",
    "    \"zh.\": \"zahradník\",\n",
    "    \"alg.\": \"algebr\",\n",
    "    \"r.\": \"rok\",\n",
    "    \"epi.\": \"epidemie\",\n",
    "    \"met.\": \"metr\",\n",
    "    \"vs.\": \"versus\",\n",
    "    \"od.\": \"odbočka\",\n",
    "    \"glu.\": \"glutamat\",\n",
    "    \"mal.\": \"malé\",\n",
    "    \"úzk.\": \"úzkost\",\n",
    "    \"dr.\": \"doktor\",\n",
    "    \"f.\": \"funkce\",\n",
    "    \"bp.\": \"blood pressure\",\n",
    "    \"sin.\": \"sinus\",\n",
    "    \"bat.\": \"baterie\",\n",
    "    \"věn.\": \"věnec\",\n",
    "    \"sep.\": \"september\",\n",
    "    \"nos.\": \"nos\",\n",
    "    \"sec.\": \"sekunda\",\n",
    "    \"op.\": \"operace\",\n",
    "    \"s.\": \"strana\",\n",
    "    \"cps.\": \"capsule\",\n",
    "    \"art.\": \"arterie\",\n",
    "    \"nem.\": \"nemoc\",\n",
    "    \"vid.\": \"vidění\",\n",
    "    \"žíl.\": \"žíla\",\n",
    "    \"poč.\": \"počet\",\n",
    "    \"ao.\": \"aorta\",\n",
    "    \"pán.\": \"pán\",\n",
    "    \"jug.\": \"jugulum\",\n",
    "    \"fem.\": \"femur\",\n",
    "    \"rad.\": \"radiace\",\n",
    "    \"ac.\": \"acidum\",\n",
    "    \"a.\": \"ampér\",\n",
    "    \"obl.\": \"oblecení\",\n",
    "    \"c.\": \"centrum\",\n",
    "    \"rok.\": \"rok\",\n",
    "    \"vel.\": \"velký\",\n",
    "    \"vag.\": \"vagón\",\n",
    "    \"med.\": \"medicína\",\n",
    "    \"kmp.\": \"kompas\",\n",
    "    \"kon.\": \"koně\",\n",
    "    \"moz.\": \"mozek\",\n",
    "    \"hoc.\": \"hodina\",\n",
    "    \"as.\": \"asistent\",\n",
    "    \"m.\": \"muž\",\n",
    "    \"urg.\": \"urgency\",\n",
    "    \"rez.\": \"rezistence\",\n",
    "    \"lym.\": \"lymfatický\",\n",
    "    \"mtb.\": \"mountain bike\",\n",
    "    \"žil.\": \"žíla\",\n",
    "    \"úsp.\": \"úspěch\",\n",
    "    \"fen.\": \"fenomén\",\n",
    "    \"deg.\": \"degree\",\n",
    "    \"ist.\": \"instalace\",\n",
    "    \"fol.\": \"folium\",\n",
    "    \"pan.\": \"paní\",\n",
    "    \"cel.\": \"celý\",\n",
    "    \"jed.\": \"jednotka\",\n",
    "    \"riz.\": \"riziko\",\n",
    "    \"rel.\": \"relativní\",\n",
    "    \"fl.\": \"flakón\",\n",
    "    \"aor.\": \"aorta\",\n",
    "    \"nod.\": \"nodus\",\n",
    "    \"abs.\": \"absolutní\",\n",
    "    \"neu.\": \"neurón\",\n",
    "    \"mit.\": \"mitoza\",\n",
    "    \"oj.\": \"ojedinělý\",\n",
    "    \"abm.\": \"abnormální\",\n",
    "    \"těl.\": \"tělo\",\n",
    "    \"tnt.\": \"trinitrotoluen\",\n",
    "    \"occ.\": \"occipital\",\n",
    "    \"h.\": \"hod\",\n",
    "    \"ekg.\": \"elektrokardiogram\",\n",
    "    \"ldx.\": \"ladění\",\n",
    "    \"typ.\": \"typ\",\n",
    "    \"kin.\": \"kinetika\",\n",
    "    \"lak.\": \"laktoza\",\n",
    "    \"box.\": \"box\",\n",
    "    \"st.\": \"stan\",\n",
    "    \"zan.\": \"zanícení\",\n",
    "    \"tel.\": \"telocvična\",\n",
    "    \"mj.\": \"mezi jinými\",\n",
    "    \"iv.\": \"intravenózní\",\n",
    "    \"dom.\": \"domácí\",\n",
    "    \"hum.\": \"humanita\",\n",
    "    \"ms.\": \"měsíc\",\n",
    "    \"syt.\": \"sytý\",\n",
    "    \"rp.\": \"republika\",\n",
    "    \"man.\": \"manuál\",\n",
    "    \"oš.\": \"ošetření\",\n",
    "    \"hem.\": \"hemoglobin\",\n",
    "    \"poz.\": \"pozice\",\n",
    "    \"akt.\": \"aktivace\",\n",
    "    \"max.\": \"maximum\",\n",
    "    \"pn.\": \"pneumatiky\",\n",
    "    \"víc.\": \"více\",\n",
    "    \"amv.\": \"amplituda\",\n",
    "    \"dg.\": \"diagnóza\",\n",
    "    \"ind.\": \"indukce\",\n",
    "    \"cor.\": \"corpora\",\n",
    "    \"atp.\": \"adenosintrifosfát\",\n",
    "    \"kyč.\": \"kyčel\",\n",
    "    \"bpn.\": \"bod per nachricht\",\n",
    "    \"d.\": \"den\",\n",
    "    \"bez.\": \"bezpečnost\",\n",
    "    \"wev.\": \"webový\",\n",
    "    \"šž.\": \"šiškova žláza\",\n",
    "    \"dex.\": \"dextróza\",\n",
    "    \"ním.\": \"němčina\",\n",
    "    \"up.\": \"upravitel\",\n",
    "    \"hil.\": \"hilium\",\n",
    "    \"rdr.\": \"radar\",\n",
    "    \"ggl.\": \"ganglion\",\n",
    "    \"ing.\": \"inženýr\",\n",
    "    \"fcí.\": \"funkce\",\n",
    "    \"čas.\": \"časopis\",\n",
    "    \"pac.\": \"pacient\",\n",
    "    \"ven.\": \"vena\",\n",
    "    \"vv.\": \"ve velkém\",\n",
    "    \"št.\": \"štětka\",\n",
    "    \"in.\": \"inklinace\",\n",
    "    \"ní.\": \"níže\",\n",
    "    \"viz.\": \"vizuální\",\n",
    "    \"cév.\": \"cévní\",\n",
    "    \"l.\": \"litera\",\n",
    "    \"ho.\": \"hodina\",\n",
    "    \"dnů.\": \"dnů\",\n",
    "    \"po.\": \"poštovní\",\n",
    "    \"mg.\": \"miligram\",\n",
    "    \"b.\": \"bod\",\n",
    "    \"hl.\": \"hlava\",\n",
    "    \"sy.\": \"systém\",\n",
    "    \"se.\": \"separace\",\n",
    "    \"ter.\": \"terapie\",\n",
    "    \"ml.\": \"mililitr\",\n",
    "    \"kg.\": \"kilogram\",\n",
    "    \"muž.\": \"muž\",\n",
    "    \"tep.\": \"teplota\",\n",
    "    \"tuk.\": \"tuk\",\n",
    "    \"hyp.\": \"hypotéza\",\n",
    "    \"inf.\": \"infekce\",\n",
    "    \"ray.\": \"radiační\",\n",
    "    \"upu.\": \"úprava\",\n",
    "    \"bil.\": \"bilance\",\n",
    "    \"cca.\": \"cirka\",\n",
    "    \"krv.\": \"krv\",\n",
    "    \"et.\": \"et cetera\",\n",
    "    \"rtp.\": \"retpolitik\",\n",
    "    \"stp.\": \"stipendium\",\n",
    "    \"výz.\": \"význam\",\n",
    "    \"ved.\": \"vedoucí\",\n",
    "    \"elf.\": \"elektrické pole\",\n",
    "    \"per.\": \"perioda\",\n",
    "    \"lig.\": \"ligament\",\n",
    "    \"fok.\": \"fokus\",\n",
    "    \"lék.\": \"lékař\",\n",
    "    \"vl.\": \"vláda\",\n",
    "    \"el.\": \"elektřina\",\n",
    "    \"tč.\": \"tečka\",\n",
    "    \"cz.\": \"čistý\",\n",
    "    \"v.\": \"vektor\",\n",
    "    \"odh.\": \"odhad\",\n",
    "    \"inh.\": \"inhalace\",\n",
    "    \"zář.\": \"záření\"\n",
    "}\n",
    "\n",
    "medical_abbreviations = {\n",
    "    \"MRI\": \"Magnetická rezonance\",\n",
    "    \"CT\": \"Počítačová tomografie\",\n",
    "    \"EKG\": \"Elektrokardiogram\",\n",
    "    \"ICU\": \"Jednotka intenzivní péče\",\n",
    "    \"CBC\": \"Kompletní krevní obraz\",\n",
    "    \"UTI\": \"Zánět močového měchýře\",\n",
    "    \"CPR\": \"Kardiopulmonální resuscitace\",\n",
    "    \"DVT\": \"Hluboká žilní trombóza\",\n",
    "    \"BMI\": \"Index tělesné hmotnosti\",\n",
    "    \"IV\": \"Intravenózní\",\n",
    "    \"ER\": \"Pohotovostní oddělení\",\n",
    "    \"BP\": \"Krevní tlak\",\n",
    "    \"CVA\": \"Cerebrovaskulární příhoda\",\n",
    "    \"ARDS\": \"Syndrom akutního respiračního selhání\",\n",
    "    \"COPD\": \"Chronická obstrukční plicní nemoc\",\n",
    "    \"BUN\": \"Urea dusík v krvi\",\n",
    "    \"GFR\": \"Glomerulární filtrační rychlost\",\n",
    "    \"TIA\": \"Tranzitorní ischemická ataka\",\n",
    "    \"PID\": \"Zánět pánevních orgánů\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Periferní cévní onemocnění\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkulóza\",\n",
    "    \"SIDS\": \"Syndrom náhlého úmrtí kojenců\",\n",
    "    \"GERD\": \"Gastroezofageální refluxní nemoc\",\n",
    "    \"ASD\": \"Atriovální septální defekt\",\n",
    "    \"ARDS\": \"Syndrom akutního respiračního selhání\",\n",
    "    \"PID\": \"Zánět pánevních orgánů\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Periferní cévní onemocnění\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkulóza\",\n",
    "    \"SIDS\": \"Syndrom náhlého úmrtí kojenců\",\n",
    "    \"GERD\": \"Gastroezofageální refluxní nemoc\",\n",
    "    \"ASD\": \"Atriovální septální defekt\",\n",
    "    \"MRI\": \"Magnetická rezonance\",\n",
    "    \"CT\": \"Počítačová tomografie\",\n",
    "    \"EKG\": \"Elektrokardiogram\",\n",
    "    \"ICU\": \"Jednotka intenzivní péče\",\n",
    "    \"CBC\": \"Kompletní krevní obraz\",\n",
    "    \"UTI\": \"Zánět močového ústrojí\",\n",
    "    \"CPR\": \"Kardiopulmonální resuscitace\",\n",
    "    \"DVT\": \"Hluboká žilní trombóza\",\n",
    "    \"BMI\": \"Index tělesné hmotnosti\",\n",
    "    \"IV\": \"Intravenózní\",\n",
    "    \"ER\": \"Pohotovostní oddělení\",\n",
    "    \"BP\": \"Krevní tlak\",\n",
    "    \"CVA\": \"Cerebrovaskulární příhoda\",\n",
    "    \"ARDS\": \"Syndrom akutního respiračního selhání\",\n",
    "    \"COPD\": \"Chronická obstrukční plicní nemoc\",\n",
    "    \"BUN\": \"Urea v krvi\",\n",
    "    \"GFR\": \"Glomerulární filtrační rychlost\",\n",
    "    \"TIA\": \"Tranzitorní ischemická ataka\",\n",
    "    \"PID\": \"Zánět pánevních orgánů\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Periferní cévní onemocnění\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkulóza\",\n",
    "    \"SIDS\": \"Syndrom náhlého úmrtí kojenců\",\n",
    "    \"GERD\": \"Gastroezofageální refluxní nemoc\",\n",
    "    \"ASD\": \"Atriovální septální defekt\",\n",
    "    \"CABG\": \"Koronární bypass operace\",\n",
    "    \"UTI\": \"Zánět močového ústrojí\",\n",
    "    \"PTSD\": \"Posttraumatická stresová porucha\",\n",
    "    \"ALS\": \"Laterální amyotrofická skleróza\",\n",
    "    \"IBS\": \"Syndrom dráždivého tračníku\",\n",
    "    \"RA\": \"Revmatoidní artritida\",\n",
    "    \"CNS\": \"Centrální nervový systém\",\n",
    "    \"DIC\": \"Diseminovaná intravaskulární koagulace\",\n",
    "    \"HIV\": \"Virus lidské imunitní nedostatečnosti\",\n",
    "    \"IDDM\": \"Insulin-dependentní diabetes mellitus\",\n",
    "    \"NSTEMI\": \"Nestabilní angina pectoris\",\n",
    "    \"STEMI\": \"Infarkt myokardu s elevací ST\",\n",
    "    \"PPE\": \"Osobní ochranné vybavení\",\n",
    "    \"IVH\": \"Intraventrikulární krvácení\",\n",
    "    \"PPE\": \"Osobní ochranné vybavení\",\n",
    "    \"IVH\": \"Intraventrikulární krvácení\",\n",
    "    # Add more abbreviations and their meanings here...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace(s):\n",
    "    for i, j in short_form_full_word.items():\n",
    "        s = s.replace(\" \" + i, \" \" + j)\n",
    "        s = s.replace(i.capitalize(), \" \" + j)\n",
    "    for i, j in medical_abbreviations.items():\n",
    "        s = s.replace(i, j)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"XX-letý pacient s perzistující FiS a flutterem síní, po opak. EKV (naposledy 10/2017), na terapii propafenonem, nyní od 10/20XX trvá SR. Echokg norm. syst. fce LK, dilat. LS - LAVi 45. Pacient při arytmii symptomatický námahovou dušností v tř. NYHA III a palpitacemi, nyní po EKV trvá SR. Pacient přijat plánovaně k RFA FiS. Echokg degenerativní změny aortální a mitrální chlopně, normální systolická funkce LKS, LAVI 29 cm3/m2. Realizována RFA pro perzist. FiS - IPŽ, RFA substrátu na přední stěně LS, bez mitrálního bloku, výkon nekomplikován. Dle kontrolní echokg bez zn. patol. perik. separace. Pacient KP komp., propuštěn do ambulantní péče.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pacient s perzistující FiS a flutterem síní, po opak. EKV naposledy / na terapii propafenonem, nyní od / trvá S rok Echokg norm. syst. fce LK, dilat. LS LAVi . Pacient při arytmii symptomatický námahovou dušností v třída NYHA III a palpitacemi, nyní po EKV trvá S rok Pacient přijat plánovaně k RFA Fi strana Echokg degenerativní změny aortální a mitrální chlopně, normální systolická funkce LKS, LAVI cm/m. Realizována RFA pro perzist. FiS IPŽ, RFA substrátu na přední stěně LS, bez mitrálního bloku, výkon nekomplikován. Dle kontrolní echokg bez znak patol. perik. separace. Pacient KP komp propuštěn do ambulantní péče.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(replace(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hospital3.csv\") \n",
    "df = df.dropna()\n",
    "# df = df.sample(5000)\n",
    "text_data = open('hospital3.txt', 'w')\n",
    "for idx, item in df.iterrows():\n",
    "  article = cleaning(replace(item[\"text\"]))\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, nltk, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.24.1 huggingface-hub-0.19.4 nltk-3.8.1 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers nltk accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " train_file_path = \"hospital3.txt\"\n",
    "model_name = 'MU-NLPC/CzeGPT-2_summarizer'\n",
    "output_dir = 'result3'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eddbd6e2b743a1b0f29172d22834ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/924k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b97d0a8533454e8a5525aa2439cda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/582k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c8dea7964f4222b091d706e2fe3299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/901 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4677f18c1ee4ddcb56e5767d868c494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21355' max='21355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21355/21355 1:44:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.818900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def generate_text(sequence, max_length):\n",
    "    model_path = \"result\"\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil priceign sinusitis maxilaris na AoS, progrese bolestí hlavy po CABG, od.. Výbornou pozici v LSI při asymptomatické supraventrikulární supraventrikulární náhradě s přiměřenou odpovědí komor, významná dilatace asc. aorty. Po implantaci D PM, po dobu monitorace SR bez arytmií, bez dysrytmií. V dobrém stavu propušten do\n"
     ]
    }
   ],
   "source": [
    "# sequence = input() # oil price\n",
    "# max_len = int(input()) # 20\n",
    "generate_text(\"oil price\", 100) # oil price for July June which had been low at as low as was originally stated Prices have since resumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helpful functions for summary generation\n",
    "# the code is a customized version of:\n",
    "#         https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py\n",
    "\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2TokenizerFast\n",
    "#from tqdm import tnrange\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def print_summary(context, gen_summary, gold_summary):\n",
    "    print('input_text', end='\\n\\n')\n",
    "    print(context, end='\\n\\n')\n",
    "    print(\"generated_summary\", end='\\n\\n')\n",
    "    print(gen_summary, end='\\n\\n')\n",
    "    print('golden_summary', end='\\n\\n')\n",
    "    print(gold_summary, end='\\n\\n')\n",
    "\n",
    "    \n",
    "def add_special_tokens(tokenizer_path):\n",
    "    \"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path, pad_token='<|endoftext|>')\n",
    "    special_tokens = {'sep_token':'<|sep|>'}\n",
    "    num_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "    # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits      \n",
    "\n",
    "\n",
    "def sample_seq_fast(model, context, length, num_sentences, device, temperature=1, top_k=0, top_p=0.0, eos_stopping=False):\n",
    "    \"\"\" Generates a sequence of tokens \n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generates one senence more than wanted\n",
    "    # looks at the generated token and estimates the num of sentences on the go\n",
    "    # after n+1 times \".!?\" it takes first n sentences by sent_tokenize\n",
    "    sent_to_gen = num_sentences + 1\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    generated = context\n",
    "    with torch.no_grad():  \n",
    "        for _ in range(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            assert len(inputs[\"input_ids\"]) <= 1024 #########################\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            if not next_token and eos_stopping:\n",
    "                break\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "            if not eos_stopping and next_token in [1, 14, 31]:\n",
    "                sent_to_gen -= 1\n",
    "                if not sent_to_gen:\n",
    "                    break\n",
    "    return generated \n",
    "\n",
    "\n",
    "def generate_summary_fast(context_enc, sep_idx, tokenizer, model, num_sentences, temperature=1, top_k=50, top_p=0.5,\n",
    "                     device=torch.device('cuda'), eos_stopping=False):\n",
    "\n",
    "    \n",
    "    # generates one senence more than wanted\n",
    "    # looks at the generated token and estimates the num of sentences on the go\n",
    "    # after n+1 times \".!?\" it takes first n sentences by sent_tokenize\n",
    "\n",
    "    generated_text = sample_seq_fast(model, context_enc, 1024-sep_idx, num_sentences, device, temperature, top_k, top_p, eos_stopping=eos_stopping)\n",
    "    generated_text = generated_text[0, len(context_enc):].tolist()\n",
    "    gen_summary = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "    gen_summary = tokenizer.convert_tokens_to_string(gen_summary)\n",
    "    \n",
    "    # extract <num_sentences> sentences \n",
    "    if not eos_stopping:\n",
    "        gen_summary.replace(\"...\", \".\")\n",
    "        try:\n",
    "            gen_summary = \" \".join(nltk.sent_tokenize(gen_summary)[:num_sentences])\n",
    "        except:\n",
    "            pass\n",
    "    return gen_summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_eval_file(data, data_type, tokenizer, model, save_dir, field, num_sentences=5,\n",
    "                       max_summaries=0, temperature=1, top_k=50, top_p=0.5, \n",
    "                       device=torch.device('cuda'), eval_step=True, eos_stopping=False, skip=0):\n",
    "    print(data_type)\n",
    "    max_summaries = math.inf if max_summaries == \"full\" else max_summaries   \n",
    "    len_data = min(max_summaries, len(data))\n",
    "    disp_len = \"full\" if max_summaries == math.inf else len_data\n",
    "    if eos_stopping:\n",
    "        save_file = save_dir + f\"/{data_type}_{disp_len}_sent{num_sentences}_eos_topk{top_k}_topp{top_p}.jsonl\"\n",
    "    else:\n",
    "        save_file = save_dir + f\"/{data_type}_{disp_len}_sent{num_sentences}_topk{top_k}_topp{top_p}.jsonl\"\n",
    "    print(f\"saving to: {save_file}\")\n",
    "    \n",
    "    how_open = \"\"\n",
    "    if skip:\n",
    "        how_open = \"a\"\n",
    "    else:\n",
    "        how_open = \"w+\"\n",
    "    with open(save_file, how_open) as output:\n",
    "        for s in range(skip, len_data): \n",
    "            if s%100 == 0:\n",
    "                print(s)\n",
    "            sample = data[s]\n",
    "            sep_idx = sample['sum_idx']\n",
    "            context = sample['input_ids'][:sep_idx].tolist()\n",
    "            gold_summary = sample['input_ids'][sep_idx+1:][:100].tolist()\n",
    "            # generating with the new faster and better method\n",
    "            gen_summary = generate_summary_fast(context, sep_idx, tokenizer, model, num_sentences, \n",
    "                             temperature=temperature, top_k=top_k, top_p=top_p, \n",
    "                             device=device, eos_stopping=eos_stopping)\n",
    "            \n",
    "            if not eval_step:\n",
    "                print_summary(tokenizer.decode(context), gen_summary, tokenizer.decode(gold_summary))\n",
    "            else:\n",
    "                new_doc = {field: gen_summary}\n",
    "                line = json\n",
    "                json.dump(new_doc, output, ensure_ascii=False)\n",
    "                output.write(\"\\n\")\n",
    "\n",
    "\n",
    "def generate_one_summary_fast(input_text, tokenizer, model, num_sentences=3,\n",
    "                        temperature=1, top_k=50, top_p=0.5,\n",
    "                        device=torch.device('cuda'), eos_stopping=False, sep_tok=True):\n",
    "\n",
    "    context = tokenizer.encode(input_text)\n",
    "    context += [tokenizer.sep_token_id]\n",
    "\n",
    "    gen_summary = generate_summary_fast(context, len(context), tokenizer, model, num_sentences, \n",
    "                                    temperature=temperature, top_k=top_k, top_p=top_p, device=device,\n",
    "                                    eos_stopping=eos_stopping)\n",
    "                \n",
    "    print_summary(tokenizer.decode(context), gen_summary, \"Not Given\")\n",
    "    \n",
    "    return gen_summary\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"result3\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' # 'cpu' alternatively\n",
    "# decice = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer_path = \"/home/ec2-user/SageMaker/tokenizer\"\n",
    "tokenizer = add_special_tokens(model_path)\n",
    "tokenizer.model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_seq = \"\"\"cleaned for github\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text\n",
      "\n",
      "cleaned for github<|sep|>\n",
      "\n",
      "generated_summary\n",
      "\n",
      "nasondovány prednisonem. Hospitalization and the clinical effusion of the patient was performed in a resting the hospital heart. Echocardiographic of the patient was performed in the pocket with clinical effusion of the patient was performed in a resting the hospital heart. The patient was performed in the pocket with clinical effusion of the pocket was performed in the pocket with pocket with pocket.\n",
      "\n",
      "golden_summary\n",
      "\n",
      "Not Given\n",
      "\n",
      "CPU times: user 1.71 s, sys: 0 ns, total: 1.71 s\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_one_summary_fast(input_seq, tokenizer, model, num_sentences=3,\n",
    "                top_k=50, top_p=0.5, device=device, eos_stopping=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
