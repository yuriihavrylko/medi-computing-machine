{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.replace(\"XX.XX.XXXX\", \" \")\n",
    "    s = s.replace(\"XX let√°\", \" \")\n",
    "    s = s.replace(\"XX-let√°\", \" \")\n",
    "    s = s.replace(\"XX-let√Ω\", \" \")\n",
    "    s = s.replace(\"XX let√Ω\", \" \")\n",
    "    s = s.replace(\"XX8let√°\", \" \")\n",
    "    s = s.replace(\"XX9let√°\", \" \")\n",
    "    s = s.replace(\"XX7let√°\", \" \")\n",
    "    s = s.replace(\"XX6let√°\", \" \")\n",
    "    s = s.replace(\"XX5let√°\", \" \")\n",
    "    s = s.replace(\"XX4let√°\", \" \")\n",
    "    s = s.replace(\"XX3let√°\", \" \")\n",
    "    s = s.replace(\"XX2let√°\", \" \")\n",
    "    s = s.replace(\"XX\", \" \")\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_form_full_word = {\n",
    "    \"tj.\": \"to jest\",\n",
    "    \"srd.\": \"srdce\",\n",
    "    \"sed.\": \"sedaƒçka\",\n",
    "    \"kad.\": \"kade≈ôn√≠k\",\n",
    "    \"rec.\": \"recepce\",\n",
    "    \"lab.\": \"laborato≈ô\",\n",
    "    \"hod.\": \"hodina\",\n",
    "    \"tik.\": \"tik√°n√≠\",\n",
    "    \"hd.\": \"hodina\",\n",
    "    \"lev.\": \"lev√Ω\",\n",
    "    \"via.\": \"viadukt\",\n",
    "    \"v√Ωn.\": \"v√Ωnos\",\n",
    "    \"abd.\": \"abdomen\",\n",
    "    \"red.\": \"redakce\",\n",
    "    \"u.\": \"ulice\",\n",
    "    \"zn.\": \"znak\",\n",
    "    \"anm.\": \"anotace\",\n",
    "    \"kys.\": \"kyselina\",\n",
    "    \"inz.\": \"in≈æen√Ωr\",\n",
    "    \"ery.\": \"erytrocyt\",\n",
    "    \"gr.\": \"gram\",\n",
    "    \"abn.\": \"abnormalita\",\n",
    "    \"fyz.\": \"fyzika\",\n",
    "    \"alb.\": \"album\",\n",
    "    \"asc.\": \"asceze\",\n",
    "    \"tam.\": \"tampon\",\n",
    "    \"c√≠l.\": \"c√≠l\",\n",
    "    \"res.\": \"rezistence\",\n",
    "    \"aa.\": \"anonymn√≠ alkoholici\",\n",
    "    \"gyn.\": \"gynokologie\",\n",
    "    \"tjn.\": \"to je\",\n",
    "    \"chl.\": \"chlazen√≠\",\n",
    "    \"soc.\": \"sociologie\",\n",
    "    \"obd.\": \"obƒõd\",\n",
    "    \"peƒç.\": \"peƒçen√≠\",\n",
    "    \"inc.\": \"inkaso\",\n",
    "    \"sp.\": \"spolek\",\n",
    "    \"den.\": \"denn√≠\",\n",
    "    \"rg.\": \"regul√°tor\",\n",
    "    \"ƒç.\": \"ƒç√≠slo\",\n",
    "    \"bol.\": \"bolest\",\n",
    "    \"i.\": \"iota\",\n",
    "    \"s√°l.\": \"s√°l\",\n",
    "    \"k.\": \"klient\",\n",
    "    \"dx.\": \"diagn√≥za\",\n",
    "    \"pat.\": \"patologie\",\n",
    "    \"exp.\": \"experiment\",\n",
    "    \"imp.\": \"import\",\n",
    "    \"inj.\": \"injekce\",\n",
    "    \"biv.\": \"bivak\",\n",
    "    \"tzn.\": \"to znamen√°\",\n",
    "    \"bnp.\": \"brain natriuretic peptide\",\n",
    "    \"z.\": \"z√°kaz\",\n",
    "    \"str.\": \"strana\",\n",
    "    \"jod.\": \"jod\",\n",
    "    \"bb.\": \"base ball\",\n",
    "    \"no.\": \"norma\",\n",
    "    \"sn.\": \"sn√≠h\",\n",
    "    \"bio.\": \"biologie\",\n",
    "    \"tis.\": \"tis√≠c\",\n",
    "    \"dyn.\": \"dynamika\",\n",
    "    \"gen.\": \"gener√°l\",\n",
    "    \"bpm.\": \"beats per minute\",\n",
    "    \"def.\": \"definice\",\n",
    "    \"sys.\": \"syst√©m\",\n",
    "    \"dig.\": \"digit√°ln√≠\",\n",
    "    \"end.\": \"endokrinologie\",\n",
    "    \"moƒç.\": \"moƒç\",\n",
    "    \"akc.\": \"akcie\",\n",
    "    \"am.\": \"amp√©r\",\n",
    "    \"z√°≈à.\": \"z√°pal\",\n",
    "    \"≈ôe≈°.\": \"≈ôe≈°en√≠\",\n",
    "    \"spt.\": \"sport\",\n",
    "    \"obj.\": \"objekt\",\n",
    "    \"doc.\": \"doktor\",\n",
    "    \"exc.\": \"excitace\",\n",
    "    \"ev.\": \"evangelium\",\n",
    "    \"mƒõs.\": \"mƒõs√≠c\",\n",
    "    \"nej.\": \"nejvy≈°≈°√≠\",\n",
    "    \"pp.\": \"popis\",\n",
    "    \"ak.\": \"akademik\",\n",
    "    \"pr.\": \"pr√°ce\",\n",
    "    \"ant.\": \"ant√©na\",\n",
    "    \"let.\": \"leti≈°tƒõ\",\n",
    "    \"adj.\": \"adjektivum\",\n",
    "    \"rod.\": \"rodina\",\n",
    "    \"os.\": \"osoba\",\n",
    "    \"map.\": \"mapa\",\n",
    "    \"pln.\": \"pln√Ω\",\n",
    "    \"tek.\": \"tekutina\",\n",
    "    \"il.\": \"ilustrace\",\n",
    "    \"acs.\": \"akutn√≠ koron√°rn√≠ syndrom\",\n",
    "    \"bok.\": \"bok\",\n",
    "    \"com.\": \"komunikace\",\n",
    "    \"amb.\": \"ambulance\",\n",
    "    \"ji≈æ.\": \"ji≈æ\",\n",
    "    \"lok.\": \"lokace\",\n",
    "    \"tbl.\": \"tabletka\",\n",
    "    \"kor.\": \"koruna\",\n",
    "    \"seq.\": \"sekvenƒçn√≠\",\n",
    "    \"dis.\": \"distribuce\",\n",
    "    \"mab.\": \"monoklon√°ln√≠ protil√°tky\",\n",
    "    \"kol.\": \"koleno\",\n",
    "    \"sv.\": \"svat√Ω\",\n",
    "    \"neg.\": \"negativn√≠\",\n",
    "    \"dop.\": \"dopis\",\n",
    "    \"zem.\": \"zemƒõ\",\n",
    "    \"gap.\": \"gap\",\n",
    "    \"dfi.\": \"deficit\",\n",
    "    \"bak.\": \"bakterie\",\n",
    "    \"kt.\": \"kategorick√Ω\",\n",
    "    \"vƒç.\": \"vƒçetnƒõ\",\n",
    "    \"vys.\": \"vysok√Ω\",\n",
    "    \"dif.\": \"diferenci√°l\",\n",
    "    \"reg.\": \"regulace\",\n",
    "    \"o.\": \"okruh\",\n",
    "    \"ung.\": \"unguentum\",\n",
    "    \"tr≈æ.\": \"tr≈æi≈°tƒõ\",\n",
    "    \"gmg.\": \"grammage\",\n",
    "    \"omp.\": \"omeprazol\",\n",
    "    \"dn√≠.\": \"dennƒõ\",\n",
    "    \"hor.\": \"hory\",\n",
    "    \"rpt.\": \"report\",\n",
    "    \"bas.\": \"basa\",\n",
    "    \"gel.\": \"gel\",\n",
    "    \"pri.\": \"prim√°rn√≠\",\n",
    "    \"aot.\": \"aorta\",\n",
    "    \"amp.\": \"amp√©r\",\n",
    "    \"opk.\": \"opravn√°\",\n",
    "    \"ca.\": \"cirka\",\n",
    "    \"n.\": \"nomenklatura\",\n",
    "    \"mut.\": \"mutace\",\n",
    "    \"by.\": \"b√Ωval√Ω\",\n",
    "    \"tib.\": \"tibia\",\n",
    "    \"hr.\": \"hranice\",\n",
    "    \"gg.\": \"geologie\",\n",
    "    \"mol.\": \"molekula\",\n",
    "    \"vit.\": \"vitam√≠n\",\n",
    "    \"dny.\": \"dna\",\n",
    "    \"sup.\": \"suplement\",\n",
    "    \"org.\": \"organizace\",\n",
    "    \"pul.\": \"pulz\",\n",
    "    \"tac.\": \"taktick√Ω\",\n",
    "    \"j√≠ƒç.\": \"j√≠cnov√Ω\",\n",
    "    \"oka.\": \"oko\",\n",
    "    \"tx.\": \"transplantace\",\n",
    "    \"j.\": \"jih\",\n",
    "    \"ch.\": \"chirurgie\",\n",
    "    \"tot.\": \"tot√°ln√≠\",\n",
    "    \"mm.\": \"milimetr\",\n",
    "    \"atb.\": \"antibiotika\",\n",
    "    \"p.\": \"para\",\n",
    "    \"min.\": \"minim√°ln√≠\",\n",
    "    \"t≈ô.\": \"t≈ô√≠da\",\n",
    "    \"trk.\": \"trafika\",\n",
    "    \"fc.\": \"football club\",\n",
    "    \"car.\": \"karcinom\",\n",
    "    \"baz.\": \"b√°ze\",\n",
    "    \"vln.\": \"vlnƒõn√≠\",\n",
    "    \"dil.\": \"dilatace\",\n",
    "    \"vr.\": \"vrcholek\",\n",
    "    \"s√≠n.\": \"s√≠nƒõ\",\n",
    "    \"ext.\": \"extremum\",\n",
    "    \"kp.\": \"kapesn√≠k\",\n",
    "    \"p≈Øv.\": \"p≈Øvod\",\n",
    "    \"t.\": \"tunel\",\n",
    "    \"ost.\": \"ostrov\",\n",
    "    \"sk.\": \"sklize≈à\",\n",
    "    \"≈°ok.\": \"≈°ok\",\n",
    "    \"st≈ô.\": \"st≈ôeda\",\n",
    "    \"sek.\": \"sekce\",\n",
    "    \"th.\": \"tƒõhotn√°\",\n",
    "    \"pl.\": \"plast\",\n",
    "    \"fib.\": \"fibula\",\n",
    "    \"bl.\": \"b√≠l√Ω\",\n",
    "    \"tƒõ≈æ.\": \"tƒõ≈æk√Ω\",\n",
    "    \"zde.\": \"zde\",\n",
    "    \"ren.\": \"renovace\",\n",
    "    \"mir.\": \"mir√°≈æ\",\n",
    "    \"on.\": \"onkologie\",\n",
    "    \"ep.\": \"epizoda\",\n",
    "    \"re.\": \"rehabilitace\",\n",
    "    \"fr.\": \"frame\",\n",
    "    \"tl.\": \"tƒõlo\",\n",
    "    \"srd.\": \"srdce\",\n",
    "    \"sed.\": \"sedadlo\",\n",
    "    \"kad.\": \"kandid√°t\",\n",
    "    \"rec.\": \"recepce\",\n",
    "    \"lab.\": \"laborato≈ô\",\n",
    "    \"hod.\": \"hodina\",\n",
    "    \"tik.\": \"tiket\",\n",
    "    \"hd.\": \"hudba\",\n",
    "    \"lev.\": \"levn√Ω\",\n",
    "    \"via.\": \"v√≠ce√∫ƒçelov√Ω\",\n",
    "    \"v√Ωn.\": \"v√Ωnosek\",\n",
    "    \"abd.\": \"abdomen\",\n",
    "    \"red.\": \"redakce\",\n",
    "    \"u.\": \"ulice\",\n",
    "    \"zn.\": \"znak\",\n",
    "    \"anm.\": \"anamn√©za\",\n",
    "    \"kys.\": \"kyselina\",\n",
    "    \"inz.\": \"in≈æen√Ωr\",\n",
    "    \"ery.\": \"erytrocyt\",\n",
    "    \"gr.\": \"gram\",\n",
    "    \"abn.\": \"abnorm√°ln√≠\",\n",
    "    \"fyz.\": \"fyzika\",\n",
    "    \"alb.\": \"albumin\",\n",
    "    \"asc.\": \"ascendentn√≠\",\n",
    "    \"tam.\": \"tampon\",\n",
    "    \"c√≠l.\": \"c√≠len√Ω\",\n",
    "    \"res.\": \"respirace\",\n",
    "    \"aa.\": \"aminokyselina\",\n",
    "    \"gyn.\": \"gynekologie\",\n",
    "    \"tjn.\": \"tajn√Ω\",\n",
    "    \"chl.\": \"chlor\",\n",
    "    \"soc.\": \"soci√°ln√≠\",\n",
    "    \"obd.\": \"obvod\",\n",
    "    \"peƒç.\": \"peƒçe≈•\",\n",
    "    \"inc.\": \"incidence\",\n",
    "    \"sp.\": \"speci√°ln√≠\",\n",
    "    \"den.\": \"den√≠k\",\n",
    "    \"rg.\": \"rentgen\",\n",
    "    \"ƒç.\": \"ƒç√≠slo\",\n",
    "    \"bol.\": \"bole\",\n",
    "    \"i.\": \"infekce\",\n",
    "    \"s√°l.\": \"s√°l\",\n",
    "    \"k.\": \"kvalita\",\n",
    "    \"dx.\": \"diagn√≥za\",\n",
    "    \"pat.\": \"patologie\",\n",
    "    \"exp.\": \"experiment\",\n",
    "    \"imp.\": \"import\",\n",
    "    \"inj.\": \"injekce\",\n",
    "    \"biv.\": \"bivalentn√≠\",\n",
    "    \"tzn.\": \"to znamen√°\",\n",
    "    \"bnp.\": \"brain natriuretic peptide\",\n",
    "    \"z.\": \"zemƒõ\",\n",
    "    \"str.\": \"str√°nka\",\n",
    "    \"jod.\": \"j√≥d\",\n",
    "    \"bb.\": \"b√≠l√° krvinka\",\n",
    "    \"no.\": \"ƒç√≠slo\",\n",
    "    \"sn.\": \"sn√≠≈æen√≠\",\n",
    "    \"bio.\": \"biologie\",\n",
    "    \"tis.\": \"tk√°≈à\",\n",
    "    \"dyn.\": \"dynamick√Ω\",\n",
    "    \"gen.\": \"generace\",\n",
    "    \"bpm.\": \"bicykly za minutu\",\n",
    "    \"def.\": \"definice\",\n",
    "    \"sys.\": \"syst√©m\",\n",
    "    \"dig.\": \"digit√°ln√≠\",\n",
    "    \"end.\": \"endokrinologie\",\n",
    "    \"moƒç.\": \"moƒç\",\n",
    "    \"akc.\": \"akce\",\n",
    "    \"am.\": \"americk√Ω\",\n",
    "    \"z√°≈à.\": \"z√°kon\",\n",
    "    \"≈ôe≈°.\": \"≈ôe≈°en√≠\",\n",
    "    \"spt.\": \"sport\",\n",
    "    \"obj.\": \"objekt\",\n",
    "    \"doc.\": \"dokument\",\n",
    "    \"exc.\": \"excelentn√≠\",\n",
    "    \"ev.\": \"event\",\n",
    "    \"mƒõs.\": \"mƒõs√≠c\",\n",
    "    \"nej.\": \"nejlep≈°√≠\",\n",
    "    \"pp.\": \"po≈°tovn√≠ p≈ôihr√°dka\",\n",
    "    \"ak.\": \"akademie\",\n",
    "    \"pr.\": \"praxe\",\n",
    "    \"ant.\": \"antibiotikum\",\n",
    "    \"let.\": \"letadlo\",\n",
    "    \"adj.\": \"adjective\",\n",
    "    \"rod.\": \"rodina\",\n",
    "    \"os.\": \"osoba\",\n",
    "    \"map.\": \"mapa\",\n",
    "    \"pln.\": \"pln√Ω\",\n",
    "    \"tek.\": \"tekutina\",\n",
    "    \"il.\": \"ilustrace\",\n",
    "    \"acs.\": \"akutn√≠ koron√°rn√≠ syndrom\",\n",
    "    \"bok.\": \"bok\",\n",
    "    \"com.\": \"komunikace\",\n",
    "    \"amb.\": \"ambulance\",\n",
    "    \"ji≈æ.\": \"ji≈æn√≠\",\n",
    "    \"lok.\": \"lokalizace\",\n",
    "    \"tbl.\": \"tabulka\",\n",
    "    \"kor.\": \"korelace\",\n",
    "    \"seq.\": \"sekvence\",\n",
    "    \"dis.\": \"dislokace\",\n",
    "    \"mab.\": \"monoklon√°ln√≠ protil√°tky\",\n",
    "    \"kol.\": \"kolegium\",\n",
    "    \"sv.\": \"svƒõtlo\",\n",
    "    \"neg.\": \"negativn√≠\",\n",
    "    \"dop.\": \"doporuƒçen√Ω\",\n",
    "    \"zem.\": \"zemƒõ\",\n",
    "    \"gap.\": \"gap\",\n",
    "    \"dfi.\": \"dƒõtsk√© fixy\",\n",
    "    \"bak.\": \"bakterie\",\n",
    "    \"kt.\": \"kultura\",\n",
    "    \"vƒç.\": \"vƒçera\",\n",
    "    \"vys.\": \"vysok√Ω\",\n",
    "    \"dif.\": \"diferenciace\",\n",
    "    \"reg.\": \"registrace\",\n",
    "    \"o.\": \"okres\",\n",
    "    \"ung.\": \"unguentum\",\n",
    "    \"tr≈æ.\": \"tr≈æba\",\n",
    "    \"gmg.\": \"gaming\",\n",
    "    \"omp.\": \"omphalos\",\n",
    "    \"dn√≠.\": \"dn√≠\",\n",
    "    \"hor.\": \"horizont\",\n",
    "    \"rpt.\": \"repeat\",\n",
    "    \"bas.\": \"basofil\",\n",
    "    \"gel.\": \"gelov√Ω\",\n",
    "    \"pri.\": \"priorita\",\n",
    "    \"aot.\": \"aorta\",\n",
    "    \"amp.\": \"amplifikace\",\n",
    "    \"opk.\": \"optick√Ω klam\",\n",
    "    \"ca.\": \"cancer\",\n",
    "    \"n.\": \"n√°rodn√≠\",\n",
    "    \"mut.\": \"mutace\",\n",
    "    \"by.\": \"bylina\",\n",
    "    \"tib.\": \"tibetsk√Ω\",\n",
    "    \"hr.\": \"horeƒçka\",\n",
    "    \"gg.\": \"good game\",\n",
    "    \"mol.\": \"molekula\",\n",
    "    \"vit.\": \"vitam√≠n\",\n",
    "    \"dny.\": \"dny\",\n",
    "    \"sup.\": \"suplement\",\n",
    "    \"org.\": \"organizace\",\n",
    "    \"pul.\": \"pulz\",\n",
    "    \"tac.\": \"tachykardie\",\n",
    "    \"j√≠c.\": \"j√≠cnov√Ω\",\n",
    "    \"oka.\": \"okam≈æitƒõ\",\n",
    "    \"tx.\": \"terapie\",\n",
    "    \"j.\": \"jednotka\",\n",
    "    \"ch.\": \"chirurgie\",\n",
    "    \"tot.\": \"totalita\",\n",
    "    \"mm.\": \"milimetr\",\n",
    "    \"atb.\": \"antibiotikum\",\n",
    "    \"p.\": \"poloha\",\n",
    "    \"min.\": \"minuta\",\n",
    "    \"t≈ô.\": \"t≈ô√≠da\",\n",
    "    \"trk.\": \"track\",\n",
    "    \"fc.\": \"fotbalov√Ω klub\",\n",
    "    \"car.\": \"carnivora\",\n",
    "    \"baz.\": \"bazilika\",\n",
    "    \"vln.\": \"vlna\",\n",
    "    \"dil.\": \"diluce\",\n",
    "    \"vr.\": \"vrstva\",\n",
    "    \"s√≠n.\": \"s√≠≈à\",\n",
    "    \"ext.\": \"extr√©mn√≠\",\n",
    "    \"kp.\": \"krevn√≠ tlak\",\n",
    "    \"p≈Øv.\": \"p≈Øvod\",\n",
    "    \"t.\": \"tento\",\n",
    "    \"ost.\": \"ostatn√≠\",\n",
    "    \"sk.\": \"sklep\",\n",
    "    \"≈°ok.\": \"≈°ok\",\n",
    "    \"st≈ô.\": \"st≈ôedn√≠\",\n",
    "    \"sek.\": \"sekunda\",\n",
    "    \"th.\": \"thorax\",\n",
    "    \"pl.\": \"pl√°≈æ\",\n",
    "    \"fib.\": \"fibrilace\",\n",
    "    \"bl.\": \"blecha\",\n",
    "    \"tƒõ≈æ.\": \"tƒõ≈æk√Ω\",\n",
    "    \"zde.\": \"zde\",\n",
    "    \"ren.\": \"ren√°ln√≠\",\n",
    "    \"mir.\": \"m√≠ra\",\n",
    "    \"on.\": \"onkologie\",\n",
    "    \"ep.\": \"epizoda\",\n",
    "    \"re.\": \"re√°ln√Ω\",\n",
    "    \"fr.\": \"frakce\",\n",
    "    \"tl.\": \"tlak\",\n",
    "    \"cm.\": \"centimetr\",\n",
    "    \"for.\": \"forenzn√≠\",\n",
    "    \"lo≈æ.\": \"lo≈ænice\",\n",
    "    \"noƒç.\": \"noƒçn√≠\",\n",
    "    \"ill.\": \"ilustrace\",\n",
    "    \"syn.\": \"synonymum\",\n",
    "    \"bic.\": \"biceps\",\n",
    "    \"krk.\": \"krk\",\n",
    "    \"fam.\": \"familie\",\n",
    "    \"dev.\": \"development\",\n",
    "    \"kr.\": \"krev\",\n",
    "    \"kom.\": \"komise\",\n",
    "    \"p≈ô.\": \"p≈ô√≠klad\",\n",
    "    \"vy≈°.\": \"vy≈°et≈ôen√≠\",\n",
    "    \"vad.\": \"vada\",\n",
    "    \"hep.\": \"hepatitida\",\n",
    "    \"spf.\": \"st≈ôedn√≠ proudov√Ω filtr\",\n",
    "    \"eti.\": \"etick√Ω\",\n",
    "    \"tok.\": \"tok\",\n",
    "    \"sat.\": \"satelit\",\n",
    "    \"gl.\": \"gl√°ndula\",\n",
    "    \"v√°g.\": \"v√°gus\",\n",
    "    \"n√°s.\": \"n√°sleduje\",\n",
    "    \"dol.\": \"doln√≠\",\n",
    "    \"inv.\": \"invaze\",\n",
    "    \"jat.\": \"jatern√≠\",\n",
    "    \"fat.\": \"f√°ze\",\n",
    "    \"ekv.\": \"ekvivalent\",\n",
    "    \"tp.\": \"typ\",\n",
    "    \"s√≠n.\": \"s√≠≈à\",\n",
    "    \"om.\": \"om√°ƒçka\",\n",
    "    \"chr.\": \"chr√°niƒç\",\n",
    "    \"do.\": \"dokonƒçit\",\n",
    "    \"lat.\": \"latinsk√Ω\",\n",
    "    \"ad.\": \"adres√°t\",\n",
    "    \"odd.\": \"odd√≠l\",\n",
    "    \"n√°m.\": \"n√°mƒõst√≠\",\n",
    "    \"kat.\": \"kategorie\",\n",
    "    \"fce.\": \"funkce\",\n",
    "    \"odp.\": \"odpovƒõƒè\",\n",
    "    \"b√≠l.\": \"b√≠l√Ω\",\n",
    "    \"por.\": \"porucha\",\n",
    "    \"noc.\": \"noc\",\n",
    "    \"bif.\": \"bifurkace\",\n",
    "    \"byl.\": \"bylinn√Ω\",\n",
    "    \"koz.\": \"koza\",\n",
    "    \"ko.\": \"kou≈ôen√≠\",\n",
    "    \"sc.\": \"sc√©na\",\n",
    "    \"≈æl.\": \"≈ælut√Ω\",\n",
    "    \"ex.\": \"exempl√°≈ô\",\n",
    "    \"tƒõ≈æ.\": \"tƒõ≈æk√Ω\",\n",
    "    \"tr.\": \"tr√©nink\",\n",
    "    \"dne.\": \"dne≈°n√≠\",\n",
    "    \"si.\": \"sice\",\n",
    "    \"sel.\": \"selektivn√≠\",\n",
    "    \"dok.\": \"dokumentace\",\n",
    "    \"osy.\": \"osy\",\n",
    "    \"amn.\": \"amen\",\n",
    "    \"kl.\": \"klid\",\n",
    "    \"pot.\": \"potenci√°l\",\n",
    "    \"int.\": \"inteligence\",\n",
    "    \"zh.\": \"zhruba\",\n",
    "    \"alg.\": \"algebra\",\n",
    "    \"r.\": \"rok\",\n",
    "    \"epi.\": \"epidemiologie\",\n",
    "    \"met.\": \"metabolismus\",\n",
    "    \"vs.\": \"versus\",\n",
    "    \"od.\": \"odezva\",\n",
    "    \"glu.\": \"gluk√≥za\",\n",
    "    \"mal.\": \"mal√Ω\",\n",
    "    \"√∫zk.\": \"√∫zkost\",\n",
    "    \"dr.\": \"doktor\",\n",
    "    \"f.\": \"funkce\",\n",
    "    \"bp.\": \"bod\",\n",
    "    \"sin.\": \"sinus\",\n",
    "    \"bat.\": \"baterie\",\n",
    "    \"ve≈à.\": \"veƒçe≈ôe\",\n",
    "    \"sep.\": \"separace\",\n",
    "    \"nos.\": \"nosn√≠\",\n",
    "    \"sec.\": \"sekunda\",\n",
    "    \"op.\": \"opakovan√Ω\",\n",
    "    \"s.\": \"souƒçasn√Ω\",\n",
    "    \"cps.\": \"cyklopentasiloxan\",\n",
    "    \"art.\": \"artikulace\",\n",
    "    \"nem.\": \"nemoc\",\n",
    "    \"vid.\": \"video\",\n",
    "    \"≈æ√≠l.\": \"≈æ√≠la\",\n",
    "    \"poƒç.\": \"poƒçet\",\n",
    "    \"ao.\": \"auto\",\n",
    "    \"jug.\": \"jugo\",\n",
    "    \"fem.\": \"femininum\",\n",
    "    \"ac.\": \"acidum\",\n",
    "    \"a.\": \"aorta\",\n",
    "    \"obl.\": \"oblast\",\n",
    "    \"c.\": \"centrum\",\n",
    "    \"rok.\": \"rokina\",\n",
    "    \"vel.\": \"velk√Ω\",\n",
    "    \"vag.\": \"vagin√°ln√≠\",\n",
    "    \"med.\": \"medic√≠na\",\n",
    "    \"kmp.\": \"kompas\",\n",
    "    \"kon.\": \"konƒçetina\",\n",
    "    \"moz.\": \"mozeƒçek\",\n",
    "    \"cm.\": \"centimetr\",\n",
    "    \"for.\": \"formace\",\n",
    "    \"lo≈æ.\": \"lo≈æisko\",\n",
    "    \"noƒç.\": \"noƒçn√≠\",\n",
    "    \"ill.\": \"ilustrace\",\n",
    "    \"syn.\": \"syn\",\n",
    "    \"bic.\": \"bic√≠ n√°stroj\",\n",
    "    \"krk.\": \"krk\",\n",
    "    \"fam.\": \"familie\",\n",
    "    \"dev.\": \"deviace\",\n",
    "    \"kr.\": \"kr√°l\",\n",
    "    \"kom.\": \"koment√°≈ô\",\n",
    "    \"p≈ô.\": \"p≈ô√≠klad\",\n",
    "    \"vy≈°.\": \"vy≈°et≈ôen√≠\",\n",
    "    \"vad.\": \"vada\",\n",
    "    \"hep.\": \"hepatitida\",\n",
    "    \"spf.\": \"sluneƒçn√≠ ochrann√Ω faktor\",\n",
    "    \"eti.\": \"etiologie\",\n",
    "    \"tok.\": \"tok\",\n",
    "    \"sat.\": \"saturace\",\n",
    "    \"gl.\": \"gluk√≥za\",\n",
    "    \"v√°g.\": \"v√°gn√≠\",\n",
    "    \"n√°s.\": \"n√°sleduj√≠c√≠\",\n",
    "    \"dol.\": \"dolar\",\n",
    "    \"inv.\": \"investice\",\n",
    "    \"jat.\": \"jatern√≠\",\n",
    "    \"fat.\": \"fat√°ln√≠\",\n",
    "    \"ekv.\": \"ekvivalent\",\n",
    "    \"tp.\": \"teplota\",\n",
    "    \"s√≠≈à.\": \"s√≠nƒõ\",\n",
    "    \"om.\": \"ohm\",\n",
    "    \"chr.\": \"chr√°m\",\n",
    "    \"do.\": \"dom√°c√≠\",\n",
    "    \"lat.\": \"latina\",\n",
    "    \"ad.\": \"advertise\",\n",
    "    \"odd.\": \"oddƒõlen√≠\",\n",
    "    \"n√°m.\": \"n√°mƒõst√≠\",\n",
    "    \"kat.\": \"kategorie\",\n",
    "    \"fce.\": \"funkce\",\n",
    "    \"odp.\": \"odpovƒõƒè\",\n",
    "    \"b√≠l.\": \"b√≠l√Ω\",\n",
    "    \"por.\": \"porada\",\n",
    "    \"noc.\": \"noc\",\n",
    "    \"bif.\": \"bifteck\",\n",
    "    \"byl.\": \"bylina\",\n",
    "    \"koz.\": \"kozel\",\n",
    "    \"ko.\": \"kobalt\",\n",
    "    \"sc.\": \"sc√©na\",\n",
    "    \"≈æl.\": \"≈æluƒç\",\n",
    "    \"ex.\": \"example\",\n",
    "    \"tƒõ≈æ.\": \"tƒõ≈æk√Ω\",\n",
    "    \"tr.\": \"tren√©r\",\n",
    "    \"dne.\": \"dnes\",\n",
    "    \"si.\": \"s√≠ran\",\n",
    "    \"sel.\": \"selen\",\n",
    "    \"dok.\": \"dokument\",\n",
    "    \"osy.\": \"osy\",\n",
    "    \"amn.\": \"amoniak\",\n",
    "    \"kl.\": \"kl√≠ƒç\",\n",
    "    \"pot.\": \"potenci√°l\",\n",
    "    \"int.\": \"intenzita\",\n",
    "    \"zh.\": \"zahradn√≠k\",\n",
    "    \"alg.\": \"algebr\",\n",
    "    \"r.\": \"rok\",\n",
    "    \"epi.\": \"epidemie\",\n",
    "    \"met.\": \"metr\",\n",
    "    \"vs.\": \"versus\",\n",
    "    \"od.\": \"odboƒçka\",\n",
    "    \"glu.\": \"glutamat\",\n",
    "    \"mal.\": \"mal√©\",\n",
    "    \"√∫zk.\": \"√∫zkost\",\n",
    "    \"dr.\": \"doktor\",\n",
    "    \"f.\": \"funkce\",\n",
    "    \"bp.\": \"blood pressure\",\n",
    "    \"sin.\": \"sinus\",\n",
    "    \"bat.\": \"baterie\",\n",
    "    \"vƒõn.\": \"vƒõnec\",\n",
    "    \"sep.\": \"september\",\n",
    "    \"nos.\": \"nos\",\n",
    "    \"sec.\": \"sekunda\",\n",
    "    \"op.\": \"operace\",\n",
    "    \"s.\": \"strana\",\n",
    "    \"cps.\": \"capsule\",\n",
    "    \"art.\": \"arterie\",\n",
    "    \"nem.\": \"nemoc\",\n",
    "    \"vid.\": \"vidƒõn√≠\",\n",
    "    \"≈æ√≠l.\": \"≈æ√≠la\",\n",
    "    \"poƒç.\": \"poƒçet\",\n",
    "    \"ao.\": \"aorta\",\n",
    "    \"p√°n.\": \"p√°n\",\n",
    "    \"jug.\": \"jugulum\",\n",
    "    \"fem.\": \"femur\",\n",
    "    \"rad.\": \"radiace\",\n",
    "    \"ac.\": \"acidum\",\n",
    "    \"a.\": \"amp√©r\",\n",
    "    \"obl.\": \"oblecen√≠\",\n",
    "    \"c.\": \"centrum\",\n",
    "    \"rok.\": \"rok\",\n",
    "    \"vel.\": \"velk√Ω\",\n",
    "    \"vag.\": \"vag√≥n\",\n",
    "    \"med.\": \"medic√≠na\",\n",
    "    \"kmp.\": \"kompas\",\n",
    "    \"kon.\": \"konƒõ\",\n",
    "    \"moz.\": \"mozek\",\n",
    "    \"hoc.\": \"hodina\",\n",
    "    \"as.\": \"asistent\",\n",
    "    \"m.\": \"mu≈æ\",\n",
    "    \"urg.\": \"urgency\",\n",
    "    \"rez.\": \"rezistence\",\n",
    "    \"lym.\": \"lymfatick√Ω\",\n",
    "    \"mtb.\": \"mountain bike\",\n",
    "    \"≈æil.\": \"≈æ√≠la\",\n",
    "    \"√∫sp.\": \"√∫spƒõch\",\n",
    "    \"fen.\": \"fenom√©n\",\n",
    "    \"deg.\": \"degree\",\n",
    "    \"ist.\": \"instalace\",\n",
    "    \"fol.\": \"folium\",\n",
    "    \"pan.\": \"pan√≠\",\n",
    "    \"cel.\": \"cel√Ω\",\n",
    "    \"jed.\": \"jednotka\",\n",
    "    \"riz.\": \"riziko\",\n",
    "    \"rel.\": \"relativn√≠\",\n",
    "    \"fl.\": \"flak√≥n\",\n",
    "    \"aor.\": \"aorta\",\n",
    "    \"nod.\": \"nodus\",\n",
    "    \"abs.\": \"absolutn√≠\",\n",
    "    \"neu.\": \"neur√≥n\",\n",
    "    \"mit.\": \"mitoza\",\n",
    "    \"oj.\": \"ojedinƒõl√Ω\",\n",
    "    \"abm.\": \"abnorm√°ln√≠\",\n",
    "    \"tƒõl.\": \"tƒõlo\",\n",
    "    \"tnt.\": \"trinitrotoluen\",\n",
    "    \"occ.\": \"occipital\",\n",
    "    \"h.\": \"hod\",\n",
    "    \"ekg.\": \"elektrokardiogram\",\n",
    "    \"ldx.\": \"ladƒõn√≠\",\n",
    "    \"typ.\": \"typ\",\n",
    "    \"kin.\": \"kinetika\",\n",
    "    \"lak.\": \"laktoza\",\n",
    "    \"box.\": \"box\",\n",
    "    \"st.\": \"stan\",\n",
    "    \"zan.\": \"zan√≠cen√≠\",\n",
    "    \"tel.\": \"telocviƒçna\",\n",
    "    \"mj.\": \"mezi jin√Ωmi\",\n",
    "    \"iv.\": \"intraven√≥zn√≠\",\n",
    "    \"dom.\": \"dom√°c√≠\",\n",
    "    \"hum.\": \"humanita\",\n",
    "    \"ms.\": \"mƒõs√≠c\",\n",
    "    \"syt.\": \"syt√Ω\",\n",
    "    \"rp.\": \"republika\",\n",
    "    \"man.\": \"manu√°l\",\n",
    "    \"o≈°.\": \"o≈°et≈ôen√≠\",\n",
    "    \"hem.\": \"hemoglobin\",\n",
    "    \"poz.\": \"pozice\",\n",
    "    \"akt.\": \"aktivace\",\n",
    "    \"max.\": \"maximum\",\n",
    "    \"pn.\": \"pneumatiky\",\n",
    "    \"v√≠c.\": \"v√≠ce\",\n",
    "    \"amv.\": \"amplituda\",\n",
    "    \"dg.\": \"diagn√≥za\",\n",
    "    \"ind.\": \"indukce\",\n",
    "    \"cor.\": \"corpora\",\n",
    "    \"atp.\": \"adenosintrifosf√°t\",\n",
    "    \"kyƒç.\": \"kyƒçel\",\n",
    "    \"bpn.\": \"bod per nachricht\",\n",
    "    \"d.\": \"den\",\n",
    "    \"bez.\": \"bezpeƒçnost\",\n",
    "    \"wev.\": \"webov√Ω\",\n",
    "    \"≈°≈æ.\": \"≈°i≈°kova ≈æl√°za\",\n",
    "    \"dex.\": \"dextr√≥za\",\n",
    "    \"n√≠m.\": \"nƒõmƒçina\",\n",
    "    \"up.\": \"upravitel\",\n",
    "    \"hil.\": \"hilium\",\n",
    "    \"rdr.\": \"radar\",\n",
    "    \"ggl.\": \"ganglion\",\n",
    "    \"ing.\": \"in≈æen√Ωr\",\n",
    "    \"fc√≠.\": \"funkce\",\n",
    "    \"ƒças.\": \"ƒçasopis\",\n",
    "    \"pac.\": \"pacient\",\n",
    "    \"ven.\": \"vena\",\n",
    "    \"vv.\": \"ve velk√©m\",\n",
    "    \"≈°t.\": \"≈°tƒõtka\",\n",
    "    \"in.\": \"inklinace\",\n",
    "    \"n√≠.\": \"n√≠≈æe\",\n",
    "    \"viz.\": \"vizu√°ln√≠\",\n",
    "    \"c√©v.\": \"c√©vn√≠\",\n",
    "    \"l.\": \"litera\",\n",
    "    \"ho.\": \"hodina\",\n",
    "    \"dn≈Ø.\": \"dn≈Ø\",\n",
    "    \"po.\": \"po≈°tovn√≠\",\n",
    "    \"mg.\": \"miligram\",\n",
    "    \"b.\": \"bod\",\n",
    "    \"hl.\": \"hlava\",\n",
    "    \"sy.\": \"syst√©m\",\n",
    "    \"se.\": \"separace\",\n",
    "    \"ter.\": \"terapie\",\n",
    "    \"ml.\": \"mililitr\",\n",
    "    \"kg.\": \"kilogram\",\n",
    "    \"mu≈æ.\": \"mu≈æ\",\n",
    "    \"tep.\": \"teplota\",\n",
    "    \"tuk.\": \"tuk\",\n",
    "    \"hyp.\": \"hypot√©za\",\n",
    "    \"inf.\": \"infekce\",\n",
    "    \"ray.\": \"radiaƒçn√≠\",\n",
    "    \"upu.\": \"√∫prava\",\n",
    "    \"bil.\": \"bilance\",\n",
    "    \"cca.\": \"cirka\",\n",
    "    \"krv.\": \"krv\",\n",
    "    \"et.\": \"et cetera\",\n",
    "    \"rtp.\": \"retpolitik\",\n",
    "    \"stp.\": \"stipendium\",\n",
    "    \"v√Ωz.\": \"v√Ωznam\",\n",
    "    \"ved.\": \"vedouc√≠\",\n",
    "    \"elf.\": \"elektrick√© pole\",\n",
    "    \"per.\": \"perioda\",\n",
    "    \"lig.\": \"ligament\",\n",
    "    \"fok.\": \"fokus\",\n",
    "    \"l√©k.\": \"l√©ka≈ô\",\n",
    "    \"vl.\": \"vl√°da\",\n",
    "    \"el.\": \"elekt≈ôina\",\n",
    "    \"tƒç.\": \"teƒçka\",\n",
    "    \"cz.\": \"ƒçist√Ω\",\n",
    "    \"v.\": \"vektor\",\n",
    "    \"odh.\": \"odhad\",\n",
    "    \"inh.\": \"inhalace\",\n",
    "    \"z√°≈ô.\": \"z√°≈ôen√≠\"\n",
    "}\n",
    "\n",
    "medical_abbreviations = {\n",
    "    \"MRI\": \"Magnetick√° rezonance\",\n",
    "    \"CT\": \"Poƒç√≠taƒçov√° tomografie\",\n",
    "    \"EKG\": \"Elektrokardiogram\",\n",
    "    \"ICU\": \"Jednotka intenzivn√≠ p√©ƒçe\",\n",
    "    \"CBC\": \"Kompletn√≠ krevn√≠ obraz\",\n",
    "    \"UTI\": \"Z√°nƒõt moƒçov√©ho mƒõch√Ω≈ôe\",\n",
    "    \"CPR\": \"Kardiopulmon√°ln√≠ resuscitace\",\n",
    "    \"DVT\": \"Hlubok√° ≈æiln√≠ tromb√≥za\",\n",
    "    \"BMI\": \"Index tƒõlesn√© hmotnosti\",\n",
    "    \"IV\": \"Intraven√≥zn√≠\",\n",
    "    \"ER\": \"Pohotovostn√≠ oddƒõlen√≠\",\n",
    "    \"BP\": \"Krevn√≠ tlak\",\n",
    "    \"CVA\": \"Cerebrovaskul√°rn√≠ p≈ô√≠hoda\",\n",
    "    \"ARDS\": \"Syndrom akutn√≠ho respiraƒçn√≠ho selh√°n√≠\",\n",
    "    \"COPD\": \"Chronick√° obstrukƒçn√≠ plicn√≠ nemoc\",\n",
    "    \"BUN\": \"Urea dus√≠k v krvi\",\n",
    "    \"GFR\": \"Glomerul√°rn√≠ filtraƒçn√≠ rychlost\",\n",
    "    \"TIA\": \"Tranzitorn√≠ ischemick√° ataka\",\n",
    "    \"PID\": \"Z√°nƒõt p√°nevn√≠ch org√°n≈Ø\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Perifern√≠ c√©vn√≠ onemocnƒõn√≠\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkul√≥za\",\n",
    "    \"SIDS\": \"Syndrom n√°hl√©ho √∫mrt√≠ kojenc≈Ø\",\n",
    "    \"GERD\": \"Gastroezofage√°ln√≠ refluxn√≠ nemoc\",\n",
    "    \"ASD\": \"Atriov√°ln√≠ sept√°ln√≠ defekt\",\n",
    "    \"ARDS\": \"Syndrom akutn√≠ho respiraƒçn√≠ho selh√°n√≠\",\n",
    "    \"PID\": \"Z√°nƒõt p√°nevn√≠ch org√°n≈Ø\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Perifern√≠ c√©vn√≠ onemocnƒõn√≠\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkul√≥za\",\n",
    "    \"SIDS\": \"Syndrom n√°hl√©ho √∫mrt√≠ kojenc≈Ø\",\n",
    "    \"GERD\": \"Gastroezofage√°ln√≠ refluxn√≠ nemoc\",\n",
    "    \"ASD\": \"Atriov√°ln√≠ sept√°ln√≠ defekt\",\n",
    "    \"MRI\": \"Magnetick√° rezonance\",\n",
    "    \"CT\": \"Poƒç√≠taƒçov√° tomografie\",\n",
    "    \"EKG\": \"Elektrokardiogram\",\n",
    "    \"ICU\": \"Jednotka intenzivn√≠ p√©ƒçe\",\n",
    "    \"CBC\": \"Kompletn√≠ krevn√≠ obraz\",\n",
    "    \"UTI\": \"Z√°nƒõt moƒçov√©ho √∫stroj√≠\",\n",
    "    \"CPR\": \"Kardiopulmon√°ln√≠ resuscitace\",\n",
    "    \"DVT\": \"Hlubok√° ≈æiln√≠ tromb√≥za\",\n",
    "    \"BMI\": \"Index tƒõlesn√© hmotnosti\",\n",
    "    \"IV\": \"Intraven√≥zn√≠\",\n",
    "    \"ER\": \"Pohotovostn√≠ oddƒõlen√≠\",\n",
    "    \"BP\": \"Krevn√≠ tlak\",\n",
    "    \"CVA\": \"Cerebrovaskul√°rn√≠ p≈ô√≠hoda\",\n",
    "    \"ARDS\": \"Syndrom akutn√≠ho respiraƒçn√≠ho selh√°n√≠\",\n",
    "    \"COPD\": \"Chronick√° obstrukƒçn√≠ plicn√≠ nemoc\",\n",
    "    \"BUN\": \"Urea v krvi\",\n",
    "    \"GFR\": \"Glomerul√°rn√≠ filtraƒçn√≠ rychlost\",\n",
    "    \"TIA\": \"Tranzitorn√≠ ischemick√° ataka\",\n",
    "    \"PID\": \"Z√°nƒõt p√°nevn√≠ch org√°n≈Ø\",\n",
    "    \"IVF\": \"In vitro fertilizace\",\n",
    "    \"PVD\": \"Perifern√≠ c√©vn√≠ onemocnƒõn√≠\",\n",
    "    \"ROM\": \"Rozsah pohybu\",\n",
    "    \"ADHD\": \"Porucha pozornosti s hyperaktivitou\",\n",
    "    \"TB\": \"Tuberkul√≥za\",\n",
    "    \"SIDS\": \"Syndrom n√°hl√©ho √∫mrt√≠ kojenc≈Ø\",\n",
    "    \"GERD\": \"Gastroezofage√°ln√≠ refluxn√≠ nemoc\",\n",
    "    \"ASD\": \"Atriov√°ln√≠ sept√°ln√≠ defekt\",\n",
    "    \"CABG\": \"Koron√°rn√≠ bypass operace\",\n",
    "    \"UTI\": \"Z√°nƒõt moƒçov√©ho √∫stroj√≠\",\n",
    "    \"PTSD\": \"Posttraumatick√° stresov√° porucha\",\n",
    "    \"ALS\": \"Later√°ln√≠ amyotrofick√° skler√≥za\",\n",
    "    \"IBS\": \"Syndrom dr√°≈ædiv√©ho traƒçn√≠ku\",\n",
    "    \"RA\": \"Revmatoidn√≠ artritida\",\n",
    "    \"CNS\": \"Centr√°ln√≠ nervov√Ω syst√©m\",\n",
    "    \"DIC\": \"Diseminovan√° intravaskul√°rn√≠ koagulace\",\n",
    "    \"HIV\": \"Virus lidsk√© imunitn√≠ nedostateƒçnosti\",\n",
    "    \"IDDM\": \"Insulin-dependentn√≠ diabetes mellitus\",\n",
    "    \"NSTEMI\": \"Nestabiln√≠ angina pectoris\",\n",
    "    \"STEMI\": \"Infarkt myokardu s elevac√≠ ST\",\n",
    "    \"PPE\": \"Osobn√≠ ochrann√© vybaven√≠\",\n",
    "    \"IVH\": \"Intraventrikul√°rn√≠ krv√°cen√≠\",\n",
    "    \"PPE\": \"Osobn√≠ ochrann√© vybaven√≠\",\n",
    "    \"IVH\": \"Intraventrikul√°rn√≠ krv√°cen√≠\",\n",
    "    # Add more abbreviations and their meanings here...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace(s):\n",
    "    for i, j in short_form_full_word.items():\n",
    "        s = s.replace(\" \" + i, \" \" + j)\n",
    "        s = s.replace(i.capitalize(), \" \" + j)\n",
    "    for i, j in medical_abbreviations.items():\n",
    "        s = s.replace(i, j)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"XX-let√Ω pacient s perzistuj√≠c√≠ FiS a flutterem s√≠n√≠, po opak. EKV (naposledy 10/2017), na terapii propafenonem, nyn√≠ od 10/20XX trv√° SR. Echokg norm. syst. fce LK, dilat. LS - LAVi 45. Pacient p≈ôi arytmii symptomatick√Ω n√°mahovou du≈°nost√≠ v t≈ô. NYHA III a palpitacemi, nyn√≠ po EKV trv√° SR. Pacient p≈ôijat pl√°novanƒõ k RFA FiS. Echokg degenerativn√≠ zmƒõny aort√°ln√≠ a mitr√°ln√≠ chlopnƒõ, norm√°ln√≠ systolick√° funkce LKS, LAVI 29 cm3/m2. Realizov√°na RFA pro perzist. FiS - IP≈Ω, RFA substr√°tu na p≈ôedn√≠ stƒõnƒõ LS, bez mitr√°ln√≠ho bloku, v√Ωkon nekomplikov√°n. Dle kontroln√≠ echokg bez zn. patol. perik. separace. Pacient KP komp., propu≈°tƒõn do ambulantn√≠ p√©ƒçe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pacient s perzistuj√≠c√≠ FiS a flutterem s√≠n√≠, po opak. EKV naposledy / na terapii propafenonem, nyn√≠ od / trv√° S rok Echokg norm. syst. fce LK, dilat. LS LAVi . Pacient p≈ôi arytmii symptomatick√Ω n√°mahovou du≈°nost√≠ v t≈ô√≠da NYHA III a palpitacemi, nyn√≠ po EKV trv√° S rok Pacient p≈ôijat pl√°novanƒõ k RFA Fi strana Echokg degenerativn√≠ zmƒõny aort√°ln√≠ a mitr√°ln√≠ chlopnƒõ, norm√°ln√≠ systolick√° funkce LKS, LAVI cm/m. Realizov√°na RFA pro perzist. FiS IP≈Ω, RFA substr√°tu na p≈ôedn√≠ stƒõnƒõ LS, bez mitr√°ln√≠ho bloku, v√Ωkon nekomplikov√°n. Dle kontroln√≠ echokg bez znak patol. perik. separace. Pacient KP komp propu≈°tƒõn do ambulantn√≠ p√©ƒçe.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(replace(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hospital3.csv\") \n",
    "df = df.dropna()\n",
    "# df = df.sample(5000)\n",
    "text_data = open('hospital3.txt', 'w')\n",
    "for idx, item in df.iterrows():\n",
    "  article = cleaning(replace(item[\"text\"]))\n",
    "  text_data.write(article)\n",
    "text_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, nltk, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.24.1 huggingface-hub-0.19.4 nltk-3.8.1 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers nltk accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " train_file_path = \"hospital3.txt\"\n",
    "model_name = 'MU-NLPC/CzeGPT-2_summarizer'\n",
    "output_dir = 'result3'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eddbd6e2b743a1b0f29172d22834ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/924k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b97d0a8533454e8a5525aa2439cda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/582k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c8dea7964f4222b091d706e2fe3299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/901 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4677f18c1ee4ddcb56e5767d868c494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21355' max='21355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21355/21355 1:44:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.818900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# It takes about 30 minutes to train in colab.\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def generate_text(sequence, max_length):\n",
    "    model_path = \"result\"\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil priceign sinusitis maxilaris na AoS, progrese bolest√≠ hlavy po CABG, od.. V√Ωbornou pozici v LSI p≈ôi asymptomatick√© supraventrikul√°rn√≠ supraventrikul√°rn√≠ n√°hradƒõ s p≈ôimƒõ≈ôenou odpovƒõd√≠ komor, v√Ωznamn√° dilatace asc. aorty. Po implantaci D PM, po dobu monitorace SR bez arytmi√≠, bez dysrytmi√≠. V dobr√©m stavu propu≈°ten do\n"
     ]
    }
   ],
   "source": [
    "# sequence = input() # oil price\n",
    "# max_len = int(input()) # 20\n",
    "generate_text(\"oil price\", 100) # oil price for July June which had been low at as low as was originally stated Prices have since resumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helpful functions for summary generation\n",
    "# the code is a customized version of:\n",
    "#         https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py\n",
    "\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2TokenizerFast\n",
    "#from tqdm import tnrange\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def print_summary(context, gen_summary, gold_summary):\n",
    "    print('input_text', end='\\n\\n')\n",
    "    print(context, end='\\n\\n')\n",
    "    print(\"generated_summary\", end='\\n\\n')\n",
    "    print(gen_summary, end='\\n\\n')\n",
    "    print('golden_summary', end='\\n\\n')\n",
    "    print(gold_summary, end='\\n\\n')\n",
    "\n",
    "    \n",
    "def add_special_tokens(tokenizer_path):\n",
    "    \"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path, pad_token='<|endoftext|>')\n",
    "    special_tokens = {'sep_token':'<|sep|>'}\n",
    "    num_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "    # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits      \n",
    "\n",
    "\n",
    "def sample_seq_fast(model, context, length, num_sentences, device, temperature=1, top_k=0, top_p=0.0, eos_stopping=False):\n",
    "    \"\"\" Generates a sequence of tokens \n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generates one senence more than wanted\n",
    "    # looks at the generated token and estimates the num of sentences on the go\n",
    "    # after n+1 times \".!?\" it takes first n sentences by sent_tokenize\n",
    "    sent_to_gen = num_sentences + 1\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    generated = context\n",
    "    with torch.no_grad():  \n",
    "        for _ in range(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            assert len(inputs[\"input_ids\"]) <= 1024 #########################\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            if not next_token and eos_stopping:\n",
    "                break\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "            if not eos_stopping and next_token in [1, 14, 31]:\n",
    "                sent_to_gen -= 1\n",
    "                if not sent_to_gen:\n",
    "                    break\n",
    "    return generated \n",
    "\n",
    "\n",
    "def generate_summary_fast(context_enc, sep_idx, tokenizer, model, num_sentences, temperature=1, top_k=50, top_p=0.5,\n",
    "                     device=torch.device('cuda'), eos_stopping=False):\n",
    "\n",
    "    \n",
    "    # generates one senence more than wanted\n",
    "    # looks at the generated token and estimates the num of sentences on the go\n",
    "    # after n+1 times \".!?\" it takes first n sentences by sent_tokenize\n",
    "\n",
    "    generated_text = sample_seq_fast(model, context_enc, 1024-sep_idx, num_sentences, device, temperature, top_k, top_p, eos_stopping=eos_stopping)\n",
    "    generated_text = generated_text[0, len(context_enc):].tolist()\n",
    "    gen_summary = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "    gen_summary = tokenizer.convert_tokens_to_string(gen_summary)\n",
    "    \n",
    "    # extract <num_sentences> sentences \n",
    "    if not eos_stopping:\n",
    "        gen_summary.replace(\"...\", \".\")\n",
    "        try:\n",
    "            gen_summary = \" \".join(nltk.sent_tokenize(gen_summary)[:num_sentences])\n",
    "        except:\n",
    "            pass\n",
    "    return gen_summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_eval_file(data, data_type, tokenizer, model, save_dir, field, num_sentences=5,\n",
    "                       max_summaries=0, temperature=1, top_k=50, top_p=0.5, \n",
    "                       device=torch.device('cuda'), eval_step=True, eos_stopping=False, skip=0):\n",
    "    print(data_type)\n",
    "    max_summaries = math.inf if max_summaries == \"full\" else max_summaries   \n",
    "    len_data = min(max_summaries, len(data))\n",
    "    disp_len = \"full\" if max_summaries == math.inf else len_data\n",
    "    if eos_stopping:\n",
    "        save_file = save_dir + f\"/{data_type}_{disp_len}_sent{num_sentences}_eos_topk{top_k}_topp{top_p}.jsonl\"\n",
    "    else:\n",
    "        save_file = save_dir + f\"/{data_type}_{disp_len}_sent{num_sentences}_topk{top_k}_topp{top_p}.jsonl\"\n",
    "    print(f\"saving to: {save_file}\")\n",
    "    \n",
    "    how_open = \"\"\n",
    "    if skip:\n",
    "        how_open = \"a\"\n",
    "    else:\n",
    "        how_open = \"w+\"\n",
    "    with open(save_file, how_open) as output:\n",
    "        for s in range(skip, len_data): \n",
    "            if s%100 == 0:\n",
    "                print(s)\n",
    "            sample = data[s]\n",
    "            sep_idx = sample['sum_idx']\n",
    "            context = sample['input_ids'][:sep_idx].tolist()\n",
    "            gold_summary = sample['input_ids'][sep_idx+1:][:100].tolist()\n",
    "            # generating with the new faster and better method\n",
    "            gen_summary = generate_summary_fast(context, sep_idx, tokenizer, model, num_sentences, \n",
    "                             temperature=temperature, top_k=top_k, top_p=top_p, \n",
    "                             device=device, eos_stopping=eos_stopping)\n",
    "            \n",
    "            if not eval_step:\n",
    "                print_summary(tokenizer.decode(context), gen_summary, tokenizer.decode(gold_summary))\n",
    "            else:\n",
    "                new_doc = {field: gen_summary}\n",
    "                line = json\n",
    "                json.dump(new_doc, output, ensure_ascii=False)\n",
    "                output.write(\"\\n\")\n",
    "\n",
    "\n",
    "def generate_one_summary_fast(input_text, tokenizer, model, num_sentences=3,\n",
    "                        temperature=1, top_k=50, top_p=0.5,\n",
    "                        device=torch.device('cuda'), eos_stopping=False, sep_tok=True):\n",
    "\n",
    "    context = tokenizer.encode(input_text)\n",
    "    context += [tokenizer.sep_token_id]\n",
    "\n",
    "    gen_summary = generate_summary_fast(context, len(context), tokenizer, model, num_sentences, \n",
    "                                    temperature=temperature, top_k=top_k, top_p=top_p, device=device,\n",
    "                                    eos_stopping=eos_stopping)\n",
    "                \n",
    "    print_summary(tokenizer.decode(context), gen_summary, \"Not Given\")\n",
    "    \n",
    "    return gen_summary\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"result3\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' # 'cpu' alternatively\n",
    "# decice = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer_path = \"/home/ec2-user/SageMaker/tokenizer\"\n",
    "tokenizer = add_special_tokens(model_path)\n",
    "tokenizer.model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_seq = \"\"\"cleaned for github\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text\n",
      "\n",
      "cleaned for github<|sep|>\n",
      "\n",
      "generated_summary\n",
      "\n",
      "nasondov√°ny prednisonem. Hospitalization and the clinical effusion of the patient was performed in a resting the hospital heart. Echocardiographic of the patient was performed in the pocket with clinical effusion of the patient was performed in a resting the hospital heart. The patient was performed in the pocket with clinical effusion of the pocket was performed in the pocket with pocket with pocket.\n",
      "\n",
      "golden_summary\n",
      "\n",
      "Not Given\n",
      "\n",
      "CPU times: user 1.71 s, sys: 0 ns, total: 1.71 s\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_one_summary_fast(input_seq, tokenizer, model, num_sentences=3,\n",
    "                top_k=50, top_p=0.5, device=device, eos_stopping=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
